train Batch:490, loss:0.03839, batch running time: 0:39
train Batch:504, loss:0.10860, batch running time: 0:23
train Batch:518, loss:0.02538, batch running time: 0:06
train Batch:532, loss:0.07144, batch running time: 0:53
train Batch:546, loss:0.19775, batch running time: 0:38
train Batch:560, loss:0.01511, batch running time: 0:24
train Batch:574, loss:0.02456, batch running time: 0:14
train Batch:588, loss:0.08586, batch running time: 0:55
train Batch:602, loss:0.25257, batch running time: 0:38
train Batch:616, loss:0.14315, batch running time: 0:26
train Batch:630, loss:0.05334, batch running time: 0:15
train Batch:644, loss:0.01281, batch running time: 0:58
train Batch:658, loss:0.03139, batch running time: 0:47
train Batch:672, loss:0.17955, batch running time: 0:27
train Batch:686, loss:0.14437, batch running time: 0:13
train Batch:700, loss:0.08961, batch running time: 0:04
train Batch:714, loss:0.37147, batch running time: 0:50
train Batch:728, loss:0.19323, batch running time: 0:36
train Batch:742, loss:0.08945, batch running time: 0:22
train Batch:756, loss:0.03934, batch running time: 0:11
train Batch:770, loss:0.38361, batch running time: 0:53
train Batch:784, loss:0.18938, batch running time: 0:39
train Batch:798, loss:0.01507, batch running time: 0:24
train Batch:812, loss:0.02083, batch running time: 0:13
train Batch:826, loss:0.16125, batch running time: 0:02
train Batch:840, loss:0.08666, batch running time: 0:44
train Batch:854, loss:0.02053, batch running time: 0:29
train Batch:868, loss:0.21870, batch running time: 0:14
train Batch:882, loss:0.03173, batch running time: 0:01
train Batch:896, loss:0.02101, batch running time: 0:44
train Batch:910, loss:0.16397, batch running time: 0:31
train Batch:924, loss:0.01341, batch running time: 0:18
train Batch:938, loss:0.14695, batch running time: 0:07
train Batch:952, loss:0.14842, batch running time: 0:51
train Batch:966, loss:0.02032, batch running time: 0:36
train Batch:980, loss:0.10472, batch running time: 0:20
train Batch:994, loss:0.00548, batch running time: 0:08
train Batch:1008, loss:0.05052, batch running time: 0:55
train Batch:1022, loss:0.13549, batch running time: 0:41
train Batch:1036, loss:0.08774, batch running time: 0:25
train Batch:1050, loss:0.18062, batch running time: 0:08
train Batch:1064, loss:0.53503, batch running time: 0:46
train Batch:1078, loss:0.06017, batch running time: 0:30
train Batch:1092, loss:0.13631, batch running time: 0:14
train Batch:1106, loss:0.18664, batch running time: 0:59
train Batch:1120, loss:0.65256, batch running time: 0:42
train Batch:1134, loss:0.08095, batch running time: 0:27
train Batch:1148, loss:0.02699, batch running time: 0:15
train Batch:1162, loss:0.15667, batch running time: 0:02
train Batch:1176, loss:0.08874, batch running time: 0:50
train Batch:1190, loss:0.30300, batch running time: 0:36
train Batch:1204, loss:0.01124, batch running time: 0:19
train Batch:1218, loss:0.55809, batch running time: 0:03
train Batch:1232, loss:0.15721, batch running time: 0:43
train Batch:1246, loss:0.09386, batch running time: 0:27
train Batch:1260, loss:0.55390, batch running time: 0:12
train Batch:1274, loss:0.01641, batch running time: 0:58
train Batch:1288, loss:0.27579, batch running time: 0:41
train Batch:1302, loss:0.23033, batch running time: 0:22
train Batch:1316, loss:0.02310, batch running time: 0:11
train Batch:1330, loss:0.02375, batch running time: 0:54
train Batch:1344, loss:0.02491, batch running time: 0:39
train Batch:1358, loss:0.22336, batch running time: 0:19
Lowest loss so far:0.15315107466364208, snapshotting weights to BestWeight-foodNonfood-res50-02-08-2019_17-07-57.pth
--------------------------------------------------
train Epoch:16, Loss: 0.1532, lr: 0.00010000, Total running time: 24:41
Epoch: 17/1300
=======================================================
train Batch:14, loss:0.01817, batch running time: 0:55
train Batch:28, loss:0.12494, batch running time: 0:38
train Batch:42, loss:0.06734, batch running time: 0:23
train Batch:56, loss:0.02007, batch running time: 0:07
train Batch:70, loss:0.12573, batch running time: 0:49
train Batch:84, loss:0.02193, batch running time: 0:34
train Batch:98, loss:0.05785, batch running time: 0:20
train Batch:112, loss:0.13030, batch running time: 0:02
train Batch:126, loss:0.04622, batch running time: 0:45
train Batch:140, loss:0.39384, batch running time: 0:32
train Batch:154, loss:0.37188, batch running time: 0:16
train Batch:168, loss:0.01160, batch running time: 0:00
train Batch:182, loss:0.17413, batch running time: 0:43
train Batch:196, loss:0.25886, batch running time: 0:30
train Batch:210, loss:0.13907, batch running time: 0:11
train Batch:224, loss:0.01216, batch running time: 0:54
train Batch:238, loss:0.02857, batch running time: 0:39
train Batch:252, loss:0.01442, batch running time: 0:23
train Batch:266, loss:0.02629, batch running time: 0:11
train Batch:280, loss:0.03290, batch running time: 0:57
train Batch:294, loss:0.11988, batch running time: 0:40
train Batch:308, loss:0.20303, batch running time: 0:25
train Batch:322, loss:0.17321, batch running time: 0:10
train Batch:336, loss:0.53039, batch running time: 0:55
train Batch:350, loss:0.02362, batch running time: 0:42
train Batch:364, loss:0.09611, batch running time: 0:25
train Batch:378, loss:0.03039, batch running time: 0:13
train Batch:392, loss:0.17537, batch running time: 0:00
train Batch:406, loss:0.01068, batch running time: 0:44
train Batch:420, loss:0.01473, batch running time: 0:34
train Batch:434, loss:0.41333, batch running time: 0:18
train Batch:448, loss:0.06239, batch running time: 0:05
train Batch:462, loss:0.02831, batch running time: 0:52
train Batch:476, loss:0.02531, batch running time: 0:39
train Batch:490, loss:0.02740, batch running time: 0:27
train Batch:504, loss:0.03056, batch running time: 0:17
train Batch:518, loss:0.00970, batch running time: 0:04
train Batch:532, loss:0.05438, batch running time: 0:46
train Batch:546, loss:0.03137, batch running time: 0:31
train Batch:560, loss:0.20039, batch running time: 0:13
train Batch:574, loss:0.24159, batch running time: 0:58
train Batch:588, loss:0.04288, batch running time: 0:43
train Batch:602, loss:0.07124, batch running time: 0:32
train Batch:616, loss:0.07122, batch running time: 0:18
train Batch:630, loss:0.04357, batch running time: 0:03
train Batch:644, loss:0.09501, batch running time: 0:45
train Batch:658, loss:0.10264, batch running time: 0:32
train Batch:672, loss:0.01858, batch running time: 0:16
train Batch:686, loss:0.06744, batch running time: 0:04
train Batch:700, loss:0.36195, batch running time: 0:43
train Batch:714, loss:0.07753, batch running time: 0:28
train Batch:728, loss:0.01889, batch running time: 0:15
train Batch:742, loss:0.26611, batch running time: 0:59
train Batch:756, loss:0.08080, batch running time: 0:45
train Batch:770, loss:0.05502, batch running time: 0:29
train Batch:784, loss:0.06053, batch running time: 0:18
train Batch:798, loss:0.06221, batch running time: 0:02
train Batch:812, loss:0.01391, batch running time: 0:49
train Batch:826, loss:0.07249, batch running time: 0:34
train Batch:840, loss:0.32376, batch running time: 0:20
train Batch:854, loss:0.02886, batch running time: 0:04
train Batch:868, loss:0.37022, batch running time: 0:49
train Batch:882, loss:0.03345, batch running time: 0:32
train Batch:896, loss:0.02841, batch running time: 0:14
train Batch:910, loss:0.66262, batch running time: 0:53
train Batch:924, loss:0.07164, batch running time: 0:34
train Batch:938, loss:0.01872, batch running time: 0:16
train Batch:952, loss:0.05051, batch running time: 0:03
train Batch:966, loss:0.11391, batch running time: 0:52
train Batch:980, loss:0.23121, batch running time: 0:38
train Batch:994, loss:0.08303, batch running time: 0:30
train Batch:1008, loss:0.06371, batch running time: 0:14
train Batch:1022, loss:0.15898, batch running time: 0:58
train Batch:1036, loss:0.70765, batch running time: 0:41
train Batch:1050, loss:0.13243, batch running time: 0:28
train Batch:1064, loss:0.35536, batch running time: 0:15
train Batch:1078, loss:0.38416, batch running time: 0:59
train Batch:1092, loss:0.27354, batch running time: 0:42
train Batch:1106, loss:0.04508, batch running time: 0:22
train Batch:1120, loss:0.09757, batch running time: 0:11
train Batch:1134, loss:0.00863, batch running time: 0:59
train Batch:1148, loss:0.03622, batch running time: 0:43
train Batch:1162, loss:0.13687, batch running time: 0:27
train Batch:1176, loss:0.39392, batch running time: 0:09
train Batch:1190, loss:0.09216, batch running time: 0:55
train Batch:1204, loss:0.62420, batch running time: 0:38
train Batch:1218, loss:0.37373, batch running time: 0:20
train Batch:1232, loss:0.40062, batch running time: 0:07
train Batch:1246, loss:0.15590, batch running time: 0:53
train Batch:1260, loss:0.18548, batch running time: 0:37
train Batch:1274, loss:0.01935, batch running time: 0:20
train Batch:1288, loss:0.12179, batch running time: 0:05
train Batch:1302, loss:0.03373, batch running time: 0:56
train Batch:1316, loss:0.31458, batch running time: 0:38
train Batch:1330, loss:0.14291, batch running time: 0:23
train Batch:1344, loss:0.29894, batch running time: 0:05
train Batch:1358, loss:0.94542, batch running time: 0:52
--------------------------------------------------
train Epoch:17, Loss: 0.1567, lr: 0.00010000, Total running time: 25:11
Epoch: 18/1300
=======================================================
train Batch:14, loss:0.04631, batch running time: 0:57
train Batch:28, loss:0.02213, batch running time: 0:46
train Batch:42, loss:0.29861, batch running time: 0:27
train Batch:56, loss:0.08596, batch running time: 0:17
train Batch:70, loss:0.07427, batch running time: 0:03
train Batch:84, loss:0.14680, batch running time: 0:51
train Batch:98, loss:0.07685, batch running time: 0:33
train Batch:112, loss:0.09093, batch running time: 0:19
train Batch:126, loss:0.17872, batch running time: 0:04
train Batch:140, loss:0.05446, batch running time: 0:46
train Batch:154, loss:0.12613, batch running time: 0:30
train Batch:168, loss:0.05689, batch running time: 0:15
train Batch:182, loss:0.06135, batch running time: 0:05
train Batch:196, loss:0.12369, batch running time: 0:51
train Batch:210, loss:0.13208, batch running time: 0:36
train Batch:224, loss:0.09836, batch running time: 0:19
train Batch:238, loss:0.31212, batch running time: 0:07
train Batch:252, loss:0.07212, batch running time: 0:56
train Batch:266, loss:0.09427, batch running time: 0:40
train Batch:280, loss:0.28214, batch running time: 0:26
train Batch:294, loss:0.07994, batch running time: 0:15
train Batch:308, loss:0.07488, batch running time: 0:02
train Batch:322, loss:0.11867, batch running time: 0:49
train Batch:336, loss:0.09577, batch running time: 0:34
train Batch:350, loss:0.03288, batch running time: 0:21
train Batch:364, loss:0.19654, batch running time: 0:07
train Batch:378, loss:0.01843, batch running time: 0:54
train Batch:392, loss:0.01719, batch running time: 0:37
train Batch:406, loss:0.02941, batch running time: 0:19
train Batch:420, loss:0.07711, batch running time: 0:01
train Batch:434, loss:0.01559, batch running time: 0:48
train Batch:448, loss:0.05921, batch running time: 0:35
train Batch:462, loss:0.36285, batch running time: 0:22
train Batch:476, loss:0.08241, batch running time: 0:09
train Batch:490, loss:0.02344, batch running time: 0:54
train Batch:504, loss:0.15225, batch running time: 0:39
train Batch:518, loss:0.28913, batch running time: 0:22
train Batch:532, loss:0.06138, batch running time: 0:08
train Batch:546, loss:0.12381, batch running time: 0:57
train Batch:560, loss:0.09560, batch running time: 0:44
train Batch:574, loss:0.11270, batch running time: 0:30
train Batch:588, loss:0.01044, batch running time: 0:14
train Batch:602, loss:0.13076, batch running time: 0:01
train Batch:616, loss:0.21991, batch running time: 0:46
train Batch:630, loss:0.02793, batch running time: 0:30
train Batch:644, loss:0.30268, batch running time: 0:11
train Batch:658, loss:0.01963, batch running time: 0:58
train Batch:672, loss:0.05734, batch running time: 0:48
train Batch:686, loss:0.12866, batch running time: 0:34
train Batch:700, loss:0.07303, batch running time: 0:22
train Batch:714, loss:0.37186, batch running time: 0:06
train Batch:728, loss:0.29952, batch running time: 0:50
train Batch:742, loss:0.04422, batch running time: 0:35
train Batch:756, loss:0.16586, batch running time: 0:20
train Batch:770, loss:0.01843, batch running time: 0:07
train Batch:784, loss:0.01197, batch running time: 0:57
train Batch:798, loss:0.18635, batch running time: 0:41
train Batch:812, loss:0.01937, batch running time: 0:27
train Batch:826, loss:0.32261, batch running time: 0:12
train Batch:840, loss:0.00549, batch running time: 0:58
train Batch:854, loss:0.00243, batch running time: 0:43
train Batch:868, loss:0.59013, batch running time: 0:30
train Batch:882, loss:0.01349, batch running time: 0:16
train Batch:896, loss:0.05117, batch running time: 0:05
train Batch:910, loss:0.14060, batch running time: 0:52
train Batch:924, loss:0.74991, batch running time: 0:38
train Batch:938, loss:0.58421, batch running time: 0:24
train Batch:952, loss:0.02393, batch running time: 0:13
train Batch:966, loss:0.05657, batch running time: 0:54
train Batch:980, loss:0.15153, batch running time: 0:39
train Batch:994, loss:0.00951, batch running time: 0:20
train Batch:1008, loss:0.02521, batch running time: 0:07
train Batch:1022, loss:0.12038, batch running time: 0:54
train Batch:1036, loss:0.10299, batch running time: 0:39
train Batch:1050, loss:0.08777, batch running time: 0:25
train Batch:1064, loss:0.02808, batch running time: 0:14
train Batch:1078, loss:0.05898, batch running time: 0:53
train Batch:1092, loss:0.04583, batch running time: 0:44
train Batch:1106, loss:0.25533, batch running time: 0:29
train Batch:1120, loss:0.26623, batch running time: 0:16
train Batch:1134, loss:0.04718, batch running time: 0:59
train Batch:1148, loss:0.05928, batch running time: 0:39
train Batch:1162, loss:0.06553, batch running time: 0:28
train Batch:1176, loss:0.06581, batch running time: 0:14
train Batch:1190, loss:1.06540, batch running time: 0:00
train Batch:1204, loss:0.01940, batch running time: 0:51
train Batch:1218, loss:0.04949, batch running time: 0:38
train Batch:1232, loss:0.08348, batch running time: 0:25
train Batch:1246, loss:0.04270, batch running time: 0:08
train Batch:1260, loss:0.05487, batch running time: 0:53
train Batch:1274, loss:0.01151, batch running time: 0:37
train Batch:1288, loss:0.23233, batch running time: 0:26
train Batch:1302, loss:0.00410, batch running time: 0:08
train Batch:1316, loss:0.45734, batch running time: 0:55
train Batch:1330, loss:0.05124, batch running time: 0:42
train Batch:1344, loss:0.12110, batch running time: 0:23
train Batch:1358, loss:0.03061, batch running time: 0:08
Lowest loss so far:0.1463958669416801, snapshotting weights to BestWeight-foodNonfood-res50-02-08-2019_17-07-57.pth
--------------------------------------------------
train Epoch:18, Loss: 0.1464, lr: 0.00010000, Total running time: 27:41
Epoch: 19/1300
=======================================================
train Batch:14, loss:0.02562, batch running time: 0:55
train Batch:28, loss:0.04611, batch running time: 0:43
train Batch:42, loss:0.03712, batch running time: 0:34
train Batch:56, loss:0.17617, batch running time: 0:20
train Batch:70, loss:0.03452, batch running time: 0:10
train Batch:84, loss:0.24678, batch running time: 0:52
train Batch:98, loss:0.34887, batch running time: 0:39
train Batch:112, loss:0.19446, batch running time: 0:25
train Batch:126, loss:0.01825, batch running time: 0:11
train Batch:140, loss:0.02030, batch running time: 0:50
train Batch:154, loss:0.20828, batch running time: 0:32
train Batch:168, loss:0.10992, batch running time: 0:19
train Batch:182, loss:0.01137, batch running time: 0:06
train Batch:196, loss:0.41730, batch running time: 0:52
train Batch:210, loss:0.01871, batch running time: 0:37
train Batch:224, loss:0.06425, batch running time: 0:24
train Batch:238, loss:0.05897, batch running time: 0:08
train Batch:252, loss:0.03233, batch running time: 0:58
train Batch:266, loss:0.26523, batch running time: 0:39
train Batch:280, loss:0.04431, batch running time: 0:28
train Batch:294, loss:0.25767, batch running time: 0:14
train Batch:308, loss:0.09171, batch running time: 0:59
train Batch:322, loss:0.14592, batch running time: 0:43
train Batch:336, loss:0.07322, batch running time: 0:31
train Batch:350, loss:0.03074, batch running time: 0:13
train Batch:364, loss:0.56733, batch running time: 0:58
train Batch:378, loss:0.12047, batch running time: 0:39
train Batch:392, loss:0.09067, batch running time: 0:23
train Batch:406, loss:0.08135, batch running time: 0:11
train Batch:420, loss:0.18289, batch running time: 0:50
train Batch:434, loss:0.59478, batch running time: 0:32
train Batch:448, loss:0.23069, batch running time: 0:20
train Batch:462, loss:0.06563, batch running time: 0:04
train Batch:476, loss:0.35830, batch running time: 0:49
train Batch:490, loss:0.17312, batch running time: 0:34
train Batch:504, loss:0.14198, batch running time: 0:19
train Batch:518, loss:0.29340, batch running time: 0:04
train Batch:532, loss:0.03082, batch running time: 0:51
train Batch:546, loss:0.08224, batch running time: 0:33
train Batch:560, loss:0.05177, batch running time: 0:20
train Batch:574, loss:0.37939, batch running time: 0:07
train Batch:588, loss:0.01677, batch running time: 0:50
train Batch:602, loss:0.28883, batch running time: 0:38
train Batch:616, loss:0.15206, batch running time: 0:26
train Batch:630, loss:0.25696, batch running time: 0:13
train Batch:644, loss:0.09720, batch running time: 0:59
train Batch:658, loss:0.00905, batch running time: 0:43
train Batch:672, loss:0.02294, batch running time: 0:30
train Batch:686, loss:0.09149, batch running time: 0:18
train Batch:700, loss:0.22724, batch running time: 0:04
train Batch:714, loss:0.06202, batch running time: 0:49
train Batch:728, loss:0.18055, batch running time: 0:35
train Batch:742, loss:0.07718, batch running time: 0:20
train Batch:756, loss:0.14170, batch running time: 0:10
train Batch:770, loss:0.05890, batch running time: 0:56
train Batch:784, loss:0.03473, batch running time: 0:41
train Batch:798, loss:0.10613, batch running time: 0:30
train Batch:812, loss:0.56998, batch running time: 0:15
train Batch:826, loss:0.23833, batch running time: 0:58
train Batch:840, loss:0.00794, batch running time: 0:46
train Batch:854, loss:0.07924, batch running time: 0:33
train Batch:868, loss:0.11548, batch running time: 0:20
train Batch:882, loss:0.03499, batch running time: 0:07
train Batch:896, loss:0.31022, batch running time: 0:55
train Batch:910, loss:0.47346, batch running time: 0:40
train Batch:924, loss:0.06989, batch running time: 0:27
train Batch:938, loss:0.03996, batch running time: 0:13
train Batch:952, loss:0.04131, batch running time: 0:54
train Batch:966, loss:0.11515, batch running time: 0:40
train Batch:980, loss:0.16198, batch running time: 0:24
train Batch:994, loss:0.09493, batch running time: 0:10
train Batch:1008, loss:0.06757, batch running time: 0:54
train Batch:1022, loss:0.05746, batch running time: 0:40
train Batch:1036, loss:0.30143, batch running time: 0:25
train Batch:1050, loss:0.08899, batch running time: 0:12
train Batch:1064, loss:0.03378, batch running time: 0:51
train Batch:1078, loss:0.68979, batch running time: 0:35
train Batch:1092, loss:0.11722, batch running time: 0:21
train Batch:1106, loss:0.31951, batch running time: 0:04
train Batch:1120, loss:0.14566, batch running time: 0:46
train Batch:1134, loss:0.02334, batch running time: 0:31
train Batch:1148, loss:0.07406, batch running time: 0:22
train Batch:1162, loss:0.03643, batch running time: 0:09
train Batch:1176, loss:0.00619, batch running time: 0:56
train Batch:1190, loss:0.07808, batch running time: 0:41
train Batch:1204, loss:0.01950, batch running time: 0:22
train Batch:1218, loss:0.15333, batch running time: 0:11
train Batch:1232, loss:0.11211, batch running time: 0:57
train Batch:1246, loss:0.02252, batch running time: 0:43
train Batch:1260, loss:0.08677, batch running time: 0:26
train Batch:1274, loss:0.75075, batch running time: 0:11
train Batch:1288, loss:0.13455, batch running time: 0:57
train Batch:1302, loss:0.05747, batch running time: 0:36
train Batch:1316, loss:0.14786, batch running time: 0:22
train Batch:1330, loss:0.24702, batch running time: 0:10
train Batch:1344, loss:0.37655, batch running time: 0:53
train Batch:1358, loss:0.15782, batch running time: 0:39
--------------------------------------------------
train Epoch:19, Loss: 0.1476, lr: 0.00010000, Total running time: 28:11
Epoch: 20/1300
=======================================================
train Batch:14, loss:0.04291, batch running time: 0:56
train Batch:28, loss:0.03406, batch running time: 0:38
train Batch:42, loss:0.04072, batch running time: 0:24
train Batch:56, loss:0.17724, batch running time: 0:11
train Batch:70, loss:0.13614, batch running time: 0:56
train Batch:84, loss:0.46057, batch running time: 0:42
train Batch:98, loss:0.16680, batch running time: 0:32
train Batch:112, loss:0.07633, batch running time: 0:21
train Batch:126, loss:0.03622, batch running time: 0:04
train Batch:140, loss:0.02981, batch running time: 0:51
train Batch:154, loss:0.11469, batch running time: 0:41
train Batch:168, loss:0.12378, batch running time: 0:26
train Batch:182, loss:0.30048, batch running time: 0:12
train Batch:196, loss:0.12280, batch running time: 0:59
train Batch:210, loss:0.19203, batch running time: 0:44
train Batch:224, loss:0.24567, batch running time: 0:30
train Batch:238, loss:0.27327, batch running time: 0:17
train Batch:252, loss:0.15815, batch running time: 0:00
train Batch:266, loss:0.01425, batch running time: 0:45
train Batch:280, loss:0.13019, batch running time: 0:35
train Batch:294, loss:0.16256, batch running time: 0:20
train Batch:308, loss:0.02500, batch running time: 0:03
train Batch:322, loss:0.10077, batch running time: 0:46
train Batch:336, loss:0.02698, batch running time: 0:30
train Batch:350, loss:0.08356, batch running time: 0:18
train Batch:364, loss:0.02106, batch running time: 0:08
train Batch:378, loss:0.02350, batch running time: 0:51
train Batch:392, loss:0.16353, batch running time: 0:34
train Batch:406, loss:0.12560, batch running time: 0:19
train Batch:420, loss:0.36079, batch running time: 0:04
train Batch:434, loss:0.18928, batch running time: 0:49
train Batch:448, loss:0.00156, batch running time: 0:31
train Batch:462, loss:0.14028, batch running time: 0:19
train Batch:476, loss:0.19845, batch running time: 0:01
train Batch:490, loss:0.02306, batch running time: 0:48
train Batch:504, loss:0.36278, batch running time: 0:32
train Batch:518, loss:0.02159, batch running time: 0:18
train Batch:532, loss:0.13865, batch running time: 0:02
train Batch:546, loss:0.04885, batch running time: 0:51
train Batch:560, loss:0.09215, batch running time: 0:41
train Batch:574, loss:0.00856, batch running time: 0:25
train Batch:588, loss:0.66921, batch running time: 0:11
train Batch:602, loss:0.64482, batch running time: 0:55
train Batch:616, loss:0.06521, batch running time: 0:44
train Batch:630, loss:0.03418, batch running time: 0:29
train Batch:644, loss:0.15360, batch running time: 0:14
train Batch:658, loss:0.18865, batch running time: 0:52
train Batch:672, loss:0.16648, batch running time: 0:36
train Batch:686, loss:0.02987, batch running time: 0:24
train Batch:700, loss:0.06022, batch running time: 0:06
train Batch:714, loss:0.34690, batch running time: 0:51
train Batch:728, loss:0.01078, batch running time: 0:37
train Batch:742, loss:0.05667, batch running time: 0:25
train Batch:756, loss:0.00900, batch running time: 0:13
train Batch:770, loss:0.02262, batch running time: 0:59
train Batch:784, loss:0.04917, batch running time: 0:42
train Batch:798, loss:0.19782, batch running time: 0:26
train Batch:812, loss:0.15339, batch running time: 0:12
train Batch:826, loss:0.08052, batch running time: 0:56
train Batch:840, loss:0.19137, batch running time: 0:41
train Batch:854, loss:0.31366, batch running time: 0:28
train Batch:868, loss:0.09396, batch running time: 0:13
train Batch:882, loss:0.29606, batch running time: 0:59
train Batch:896, loss:0.01154, batch running time: 0:47
train Batch:910, loss:0.10567, batch running time: 0:30
train Batch:924, loss:0.14534, batch running time: 0:16
train Batch:938, loss:0.27174, batch running time: 0:04
train Batch:952, loss:0.41475, batch running time: 0:51
train Batch:966, loss:0.07050, batch running time: 0:37
train Batch:980, loss:0.82677, batch running time: 0:25
train Batch:994, loss:0.02610, batch running time: 0:10
train Batch:1008, loss:0.08619, batch running time: 0:01
train Batch:1022, loss:0.04300, batch running time: 0:45
train Batch:1036, loss:0.00177, batch running time: 0:34
train Batch:1050, loss:0.01573, batch running time: 0:17
train Batch:1064, loss:0.05598, batch running time: 0:05
train Batch:1078, loss:0.65722, batch running time: 0:51
train Batch:1092, loss:0.17591, batch running time: 0:38
train Batch:1106, loss:0.24399, batch running time: 0:21
train Batch:1120, loss:0.58449, batch running time: 0:06
train Batch:1134, loss:0.06633, batch running time: 0:54
train Batch:1148, loss:0.17328, batch running time: 0:40
train Batch:1162, loss:0.01337, batch running time: 0:26
train Batch:1176, loss:0.39711, batch running time: 0:10
train Batch:1190, loss:0.19224, batch running time: 0:57
train Batch:1204, loss:0.05002, batch running time: 0:41
train Batch:1218, loss:0.04733, batch running time: 0:28
train Batch:1232, loss:0.01748, batch running time: 0:13
train Batch:1246, loss:0.49084, batch running time: 0:00
train Batch:1260, loss:0.01143, batch running time: 0:47
train Batch:1274, loss:0.02073, batch running time: 0:33
train Batch:1288, loss:0.26335, batch running time: 0:19
train Batch:1302, loss:0.15185, batch running time: 0:05
train Batch:1316, loss:0.19882, batch running time: 0:54
train Batch:1330, loss:0.01641, batch running time: 0:36
train Batch:1344, loss:0.16206, batch running time: 0:23
train Batch:1358, loss:0.00871, batch running time: 0:10
Lowest loss so far:0.1407860983103347, snapshotting weights to BestWeight-foodNonfood-res50-02-08-2019_17-07-57.pth
--------------------------------------------------
train Epoch:20, Loss: 0.1408, lr: 0.00010000, Total running time: 30:41
Epoch: 21/1300
=======================================================
train Batch:14, loss:0.07734, batch running time: 0:54
train Batch:28, loss:0.08186, batch running time: 0:39
train Batch:42, loss:0.78336, batch running time: 0:20
train Batch:56, loss:0.03720, batch running time: 0:11
train Batch:70, loss:0.27092, batch running time: 0:57
train Batch:84, loss:0.07218, batch running time: 0:43
train Batch:98, loss:0.04083, batch running time: 0:31
train Batch:112, loss:0.42073, batch running time: 0:16
train Batch:126, loss:0.02938, batch running time: 0:01
train Batch:140, loss:0.01133, batch running time: 0:48
train Batch:154, loss:0.06153, batch running time: 0:29
train Batch:168, loss:0.08148, batch running time: 0:13
train Batch:182, loss:0.01180, batch running time: 0:02
train Batch:196, loss:0.24149, batch running time: 0:45
train Batch:210, loss:0.30287, batch running time: 0:32
train Batch:224, loss:0.01350, batch running time: 0:22
train Batch:238, loss:0.00872, batch running time: 0:08
train Batch:252, loss:0.09728, batch running time: 0:53
train Batch:266, loss:0.08524, batch running time: 0:38
train Batch:280, loss:0.04825, batch running time: 0:22
train Batch:294, loss:0.07246, batch running time: 0:11
train Batch:308, loss:0.09257, batch running time: 0:58
train Batch:322, loss:0.29986, batch running time: 0:43
train Batch:336, loss:0.20139, batch running time: 0:30
train Batch:350, loss:0.31528, batch running time: 0:16
train Batch:364, loss:0.05427, batch running time: 0:01
train Batch:378, loss:0.22180, batch running time: 0:47
train Batch:392, loss:0.18443, batch running time: 0:32
train Batch:406, loss:0.05460, batch running time: 0:17
train Batch:420, loss:0.06641, batch running time: 0:04
train Batch:434, loss:0.01439, batch running time: 0:48
train Batch:448, loss:0.46847, batch running time: 0:34
train Batch:462, loss:0.18702, batch running time: 0:22
train Batch:476, loss:0.04765, batch running time: 0:04
train Batch:490, loss:0.22833, batch running time: 0:47
train Batch:504, loss:0.75334, batch running time: 0:30
train Batch:518, loss:0.48627, batch running time: 0:11
train Batch:532, loss:0.14692, batch running time: 0:00
train Batch:546, loss:0.14841, batch running time: 0:44
train Batch:560, loss:0.46280, batch running time: 0:25
train Batch:574, loss:0.39228, batch running time: 0:09
train Batch:588, loss:0.05814, batch running time: 0:55
train Batch:602, loss:0.15800, batch running time: 0:46
train Batch:616, loss:0.11647, batch running time: 0:33
train Batch:630, loss:0.17114, batch running time: 0:22
train Batch:644, loss:0.23304, batch running time: 0:08
train Batch:658, loss:0.16032, batch running time: 0:54
train Batch:672, loss:0.04820, batch running time: 0:41
train Batch:686, loss:0.07446, batch running time: 0:31
train Batch:700, loss:0.01168, batch running time: 0:14
train Batch:714, loss:0.27227, batch running time: 0:56
train Batch:728, loss:0.01054, batch running time: 0:42
train Batch:742, loss:0.03317, batch running time: 0:27
train Batch:756, loss:0.28195, batch running time: 0:12
train Batch:770, loss:0.23442, batch running time: 0:58
train Batch:784, loss:0.13361, batch running time: 0:44
train Batch:798, loss:0.06323, batch running time: 0:33
train Batch:812, loss:0.00774, batch running time: 0:18
train Batch:826, loss:0.19903, batch running time: 0:06
train Batch:840, loss:0.05176, batch running time: 0:51
train Batch:854, loss:0.26686, batch running time: 0:34
train Batch:868, loss:0.16246, batch running time: 0:17
train Batch:882, loss:0.13867, batch running time: 0:06
train Batch:896, loss:0.16115, batch running time: 0:51
train Batch:910, loss:0.01643, batch running time: 0:39
train Batch:924, loss:0.16762, batch running time: 0:24
train Batch:938, loss:0.02941, batch running time: 0:09
train Batch:952, loss:0.05121, batch running time: 0:51
train Batch:966, loss:0.21081, batch running time: 0:37
train Batch:980, loss:0.13282, batch running time: 0:23
train Batch:994, loss:0.17551, batch running time: 0:11
train Batch:1008, loss:0.01592, batch running time: 0:55
train Batch:1022, loss:0.19088, batch running time: 0:42
train Batch:1036, loss:0.04326, batch running time: 0:26
train Batch:1050, loss:0.20555, batch running time: 0:12
train Batch:1064, loss:0.34122, batch running time: 0:01
train Batch:1078, loss:0.54398, batch running time: 0:44
train Batch:1092, loss:0.06024, batch running time: 0:32
train Batch:1106, loss:0.00737, batch running time: 0:17
train Batch:1120, loss:0.09627, batch running time: 0:04
train Batch:1134, loss:0.09356, batch running time: 0:53
train Batch:1148, loss:0.13300, batch running time: 0:39
train Batch:1162, loss:0.15564, batch running time: 0:26
train Batch:1176, loss:0.32456, batch running time: 0:12
train Batch:1190, loss:0.33137, batch running time: 0:00
train Batch:1204, loss:0.00742, batch running time: 0:47
train Batch:1218, loss:0.09489, batch running time: 0:35
train Batch:1232, loss:0.50180, batch running time: 0:15
train Batch:1246, loss:0.10182, batch running time: 0:03
train Batch:1260, loss:0.22004, batch running time: 0:47
train Batch:1274, loss:0.43116, batch running time: 0:34
train Batch:1288, loss:0.06730, batch running time: 0:23
train Batch:1302, loss:0.52158, batch running time: 0:10
train Batch:1316, loss:0.46658, batch running time: 0:59
train Batch:1330, loss:0.05575, batch running time: 0:46
train Batch:1344, loss:0.06481, batch running time: 0:33
train Batch:1358, loss:0.07378, batch running time: 0:21
Lowest loss so far:0.13762121561730586, snapshotting weights to BestWeight-foodNonfood-res50-02-08-2019_17-07-57.pth
--------------------------------------------------
train Epoch:21, Loss: 0.1376, lr: 0.00010000, Total running time: 31:11
Epoch: 22/1300
=======================================================
train Batch:14, loss:0.19775, batch running time: 0:57
train Batch:28, loss:0.12806, batch running time: 0:45
train Batch:42, loss:0.28462, batch running time: 0:31
train Batch:56, loss:0.04712, batch running time: 0:17
train Batch:70, loss:0.09113, batch running time: 0:07
train Batch:84, loss:0.18683, batch running time: 0:51
train Batch:98, loss:0.01022, batch running time: 0:36
train Batch:112, loss:0.43721, batch running time: 0:19
train Batch:126, loss:0.02682, batch running time: 0:09
train Batch:140, loss:0.10635, batch running time: 0:52
train Batch:154, loss:0.02853, batch running time: 0:38
train Batch:168, loss:0.03517, batch running time: 0:27
train Batch:182, loss:0.01061, batch running time: 0:15
train Batch:196, loss:0.06169, batch running time: 0:58
train Batch:210, loss:0.50952, batch running time: 0:45
train Batch:224, loss:0.15195, batch running time: 0:32
train Batch:238, loss:0.03583, batch running time: 0:20
train Batch:252, loss:0.55065, batch running time: 0:04
train Batch:266, loss:0.29782, batch running time: 0:50
train Batch:280, loss:0.01341, batch running time: 0:34
train Batch:294, loss:0.11450, batch running time: 0:22
train Batch:308, loss:0.78795, batch running time: 0:07
train Batch:322, loss:0.02369, batch running time: 0:52
train Batch:336, loss:0.02532, batch running time: 0:39
train Batch:350, loss:0.02186, batch running time: 0:26
train Batch:364, loss:0.03018, batch running time: 0:11
train Batch:378, loss:0.03366, batch running time: 0:54
train Batch:392, loss:0.02450, batch running time: 0:37
train Batch:406, loss:0.02267, batch running time: 0:18
train Batch:420, loss:0.32776, batch running time: 0:02
train Batch:434, loss:0.20260, batch running time: 0:47
train Batch:448, loss:0.06667, batch running time: 0:35
train Batch:462, loss:0.11502, batch running time: 0:22
train Batch:476, loss:0.05993, batch running time: 0:05
train Batch:490, loss:0.65075, batch running time: 0:54
train Batch:504, loss:0.04220, batch running time: 0:45
train Batch:518, loss:0.01721, batch running time: 0:33
train Batch:532, loss:0.04799, batch running time: 0:21
train Batch:546, loss:0.17136, batch running time: 0:07
train Batch:560, loss:0.08627, batch running time: 0:55
train Batch:574, loss:0.07944, batch running time: 0:42
train Batch:588, loss:0.02650, batch running time: 0:24
train Batch:602, loss:0.36654, batch running time: 0:05
train Batch:616, loss:0.29306, batch running time: 0:54
train Batch:630, loss:0.10997, batch running time: 0:41
train Batch:644, loss:0.07021, batch running time: 0:27
train Batch:658, loss:0.01084, batch running time: 0:14
train Batch:672, loss:0.19143, batch running time: 0:59
train Batch:686, loss:0.05175, batch running time: 0:47
train Batch:700, loss:0.47866, batch running time: 0:35
train Batch:714, loss:0.02920, batch running time: 0:19
train Batch:728, loss:0.07882, batch running time: 0:07
train Batch:742, loss:0.15274, batch running time: 0:53
train Batch:756, loss:0.28192, batch running time: 0:38
train Batch:770, loss:0.05879, batch running time: 0:29
train Batch:784, loss:0.00936, batch running time: 0:17
train Batch:798, loss:0.02436, batch running time: 0:00
train Batch:812, loss:0.14602, batch running time: 0:47
train Batch:826, loss:0.25244, batch running time: 0:32
train Batch:840, loss:0.14118, batch running time: 0:20
train Batch:854, loss:0.64269, batch running time: 0:04
train Batch:868, loss:0.41258, batch running time: 0:48
train Batch:882, loss:0.21318, batch running time: 0:36
train Batch:896, loss:0.01799, batch running time: 0:23
train Batch:910, loss:0.01062, batch running time: 0:10
train Batch:924, loss:0.47420, batch running time: 0:55
train Batch:938, loss:0.36918, batch running time: 0:39
train Batch:952, loss:0.15445, batch running time: 0:24
train Batch:966, loss:0.35257, batch running time: 0:11
train Batch:980, loss:0.23746, batch running time: 0:53
train Batch:994, loss:0.07760, batch running time: 0:38
train Batch:1008, loss:0.24220, batch running time: 0:21
train Batch:1022, loss:0.22070, batch running time: 0:03
train Batch:1036, loss:0.07806, batch running time: 0:49
train Batch:1050, loss:0.43506, batch running time: 0:33
train Batch:1064, loss:0.02171, batch running time: 0:20
train Batch:1078, loss:0.03657, batch running time: 0:08
train Batch:1092, loss:0.10557, batch running time: 0:56
train Batch:1106, loss:0.16735, batch running time: 0:46
train Batch:1120, loss:0.04011, batch running time: 0:33
train Batch:1134, loss:0.07793, batch running time: 0:18
train Batch:1148, loss:0.25302, batch running time: 0:03
train Batch:1162, loss:0.09666, batch running time: 0:47
train Batch:1176, loss:0.06874, batch running time: 0:35
train Batch:1190, loss:0.00950, batch running time: 0:25
train Batch:1204, loss:0.06293, batch running time: 0:11
train Batch:1218, loss:0.02625, batch running time: 0:54
train Batch:1232, loss:0.01587, batch running time: 0:43
train Batch:1246, loss:0.03468, batch running time: 0:27
train Batch:1260, loss:0.12591, batch running time: 0:11
train Batch:1274, loss:0.14938, batch running time: 0:57
train Batch:1288, loss:0.13698, batch running time: 0:38
train Batch:1302, loss:0.00698, batch running time: 0:24
train Batch:1316, loss:0.04885, batch running time: 0:09
train Batch:1330, loss:0.00490, batch running time: 0:52
train Batch:1344, loss:0.03377, batch running time: 0:40
train Batch:1358, loss:0.29430, batch running time: 0:28
Lowest loss so far:0.13497722519955627, snapshotting weights to BestWeight-foodNonfood-res50-02-08-2019_17-07-57.pth
--------------------------------------------------
train Epoch:22, Loss: 0.1350, lr: 0.00010000, Total running time: 33:41
Epoch: 23/1300
=======================================================
train Batch:14, loss:0.01525, batch running time: 0:52
train Batch:28, loss:0.36015, batch running time: 0:37
train Batch:42, loss:0.07249, batch running time: 0:28
train Batch:56, loss:0.05631, batch running time: 0:15
train Batch:70, loss:0.14693, batch running time: 0:59
train Batch:84, loss:0.30252, batch running time: 0:40
train Batch:98, loss:0.06621, batch running time: 0:28
train Batch:112, loss:0.05631, batch running time: 0:14
train Batch:126, loss:0.31035, batch running time: 0:00
train Batch:140, loss:0.24379, batch running time: 0:45
train Batch:154, loss:0.03188, batch running time: 0:33
train Batch:168, loss:0.03142, batch running time: 0:21
train Batch:182, loss:0.08330, batch running time: 0:10
train Batch:196, loss:0.07999, batch running time: 0:55
train Batch:210, loss:0.18638, batch running time: 0:44
train Batch:224, loss:0.10890, batch running time: 0:27
train Batch:238, loss:0.26579, batch running time: 0:13
train Batch:252, loss:0.08665, batch running time: 0:01
train Batch:266, loss:0.46671, batch running time: 0:48
train Batch:280, loss:0.31094, batch running time: 0:37
train Batch:294, loss:0.15267, batch running time: 0:25
train Batch:308, loss:0.70488, batch running time: 0:09
train Batch:322, loss:0.33038, batch running time: 0:56
train Batch:336, loss:0.05500, batch running time: 0:40
train Batch:350, loss:0.66990, batch running time: 0:20
train Batch:364, loss:0.13912, batch running time: 0:06
train Batch:378, loss:0.43968, batch running time: 0:50
train Batch:392, loss:0.46378, batch running time: 0:34
train Batch:406, loss:0.01585, batch running time: 0:19
train Batch:420, loss:0.03039, batch running time: 0:04
train Batch:434, loss:0.20583, batch running time: 0:48
train Batch:448, loss:0.01493, batch running time: 0:36
train Batch:462, loss:0.08118, batch running time: 0:23
train Batch:476, loss:0.00332, batch running time: 0:10
train Batch:490, loss:0.01605, batch running time: 0:57
train Batch:504, loss:0.02112, batch running time: 0:48
train Batch:518, loss:0.22508, batch running time: 0:30
train Batch:532, loss:0.01833, batch running time: 0:15
train Batch:546, loss:0.01225, batch running time: 0:02
train Batch:560, loss:0.03238, batch running time: 0:48
train Batch:574, loss:0.12501, batch running time: 0:38
train Batch:588, loss:0.00996, batch running time: 0:27
train Batch:602, loss:0.27430, batch running time: 0:15
train Batch:616, loss:0.39286, batch running time: 0:01
train Batch:630, loss:0.07588, batch running time: 0:49
train Batch:644, loss:0.00988, batch running time: 0:37
train Batch:658, loss:0.05755, batch running time: 0:28
train Batch:672, loss:0.01774, batch running time: 0:16
train Batch:686, loss:0.01519, batch running time: 0:03
train Batch:700, loss:0.12040, batch running time: 0:50
train Batch:714, loss:0.01585, batch running time: 0:35
train Batch:728, loss:0.10383, batch running time: 0:19
train Batch:742, loss:0.14912, batch running time: 0:04
train Batch:756, loss:0.24747, batch running time: 0:52
train Batch:770, loss:0.06940, batch running time: 0:39
train Batch:784, loss:0.07006, batch running time: 0:25
train Batch:798, loss:0.08274, batch running time: 0:15
train Batch:812, loss:0.14951, batch running time: 0:03
train Batch:826, loss:0.05063, batch running time: 0:53
train Batch:840, loss:0.01488, batch running time: 0:40
train Batch:854, loss:0.50125, batch running time: 0:26
train Batch:868, loss:0.02549, batch running time: 0:15
train Batch:882, loss:0.05979, batch running time: 0:02
train Batch:896, loss:0.00866, batch running time: 0:47
train Batch:910, loss:0.02069, batch running time: 0:31
train Batch:924, loss:0.59805, batch running time: 0:19
train Batch:938, loss:0.00940, batch running time: 0:03
train Batch:952, loss:0.09201, batch running time: 0:48
train Batch:966, loss:0.13722, batch running time: 0:32
train Batch:980, loss:0.19307, batch running time: 0:17
train Batch:994, loss:0.05132, batch running time: 0:05
train Batch:1008, loss:0.01510, batch running time: 0:52
train Batch:1022, loss:0.10945, batch running time: 0:36
train Batch:1036, loss:0.16337, batch running time: 0:17
train Batch:1050, loss:0.01857, batch running time: 0:06
train Batch:1064, loss:0.22565, batch running time: 0:54
train Batch:1078, loss:0.00232, batch running time: 0:44
train Batch:1092, loss:0.04183, batch running time: 0:30
train Batch:1106, loss:0.02733, batch running time: 0:18
train Batch:1120, loss:0.05148, batch running time: 0:05
train Batch:1134, loss:0.13155, batch running time: 0:52
train Batch:1148, loss:0.15741, batch running time: 0:40
train Batch:1162, loss:0.12605, batch running time: 0:27
train Batch:1176, loss:0.01648, batch running time: 0:14
train Batch:1190, loss:0.29852, batch running time: 0:00
train Batch:1204, loss:0.05877, batch running time: 0:50
train Batch:1218, loss:0.10821, batch running time: 0:38
train Batch:1232, loss:0.61666, batch running time: 0:25
train Batch:1246, loss:0.04908, batch running time: 0:04
train Batch:1260, loss:0.12225, batch running time: 0:52
train Batch:1274, loss:0.06745, batch running time: 0:39
train Batch:1288, loss:0.02107, batch running time: 0:24
train Batch:1302, loss:0.24692, batch running time: 0:07
train Batch:1316, loss:0.00996, batch running time: 0:56
train Batch:1330, loss:0.03164, batch running time: 0:44
train Batch:1344, loss:0.24639, batch running time: 0:25
train Batch:1358, loss:0.03288, batch running time: 0:10
Lowest loss so far:0.13181296490700464, snapshotting weights to BestWeight-foodNonfood-res50-02-08-2019_17-07-57.pth
--------------------------------------------------
train Epoch:23, Loss: 0.1318, lr: 0.00010000, Total running time: 34:11
Epoch: 24/1300
=======================================================
train Batch:14, loss:0.03348, batch running time: 0:58
train Batch:28, loss:0.23188, batch running time: 0:45
train Batch:42, loss:0.04800, batch running time: 0:35
train Batch:56, loss:0.05337, batch running time: 0:22
train Batch:70, loss:0.02893, batch running time: 0:11
train Batch:84, loss:0.05661, batch running time: 0:58
train Batch:98, loss:0.11009, batch running time: 0:44
train Batch:112, loss:0.69753, batch running time: 0:30
train Batch:126, loss:0.05817, batch running time: 0:17
train Batch:140, loss:0.33295, batch running time: 0:06
train Batch:154, loss:0.20248, batch running time: 0:51
train Batch:168, loss:0.03752, batch running time: 0:35
train Batch:182, loss:0.02527, batch running time: 0:23
train Batch:196, loss:0.11970, batch running time: 0:12
train Batch:210, loss:0.06868, batch running time: 0:53
train Batch:224, loss:0.01921, batch running time: 0:38
train Batch:238, loss:0.00694, batch running time: 0:26
train Batch:252, loss:0.04094, batch running time: 0:10
train Batch:266, loss:0.11456, batch running time: 0:58
train Batch:280, loss:0.01835, batch running time: 0:44
train Batch:294, loss:0.15654, batch running time: 0:24
train Batch:308, loss:0.01359, batch running time: 0:12
train Batch:322, loss:0.01035, batch running time: 0:56
train Batch:336, loss:0.00573, batch running time: 0:45
train Batch:350, loss:0.03485, batch running time: 0:29
train Batch:364, loss:0.00960, batch running time: 0:15
train Batch:378, loss:0.21860, batch running time: 0:00
train Batch:392, loss:0.01553, batch running time: 0:49
train Batch:406, loss:0.03391, batch running time: 0:36
train Batch:420, loss:0.27511, batch running time: 0:22
train Batch:434, loss:0.03212, batch running time: 0:06
train Batch:448, loss:0.00978, batch running time: 0:53
train Batch:462, loss:0.31551, batch running time: 0:42
train Batch:476, loss:0.24455, batch running time: 0:27
train Batch:490, loss:0.02980, batch running time: 0:14
train Batch:504, loss:0.09863, batch running time: 0:01
train Batch:518, loss:0.03920, batch running time: 0:50
train Batch:532, loss:0.05567, batch running time: 0:29
train Batch:546, loss:0.08335, batch running time: 0:18
train Batch:560, loss:0.40422, batch running time: 0:04
train Batch:574, loss:0.03576, batch running time: 0:50
train Batch:588, loss:0.01170, batch running time: 0:37
train Batch:602, loss:0.10991, batch running time: 0:22
train Batch:616, loss:0.26810, batch running time: 0:07
train Batch:630, loss:0.18794, batch running time: 0:50
train Batch:644, loss:0.04737, batch running time: 0:40
train Batch:658, loss:0.06303, batch running time: 0:25
train Batch:672, loss:0.11009, batch running time: 0:06
train Batch:686, loss:0.51219, batch running time: 0:50
train Batch:700, loss:0.09073, batch running time: 0:37
train Batch:714, loss:0.04907, batch running time: 0:21
train Batch:728, loss:0.01780, batch running time: 0:06
train Batch:742, loss:0.46433, batch running time: 0:52
train Batch:756, loss:0.12333, batch running time: 0:35
train Batch:770, loss:0.07730, batch running time: 0:17
train Batch:784, loss:0.00812, batch running time: 0:02
train Batch:798, loss:0.07307, batch running time: 0:48
train Batch:812, loss:0.45316, batch running time: 0:35
train Batch:826, loss:0.02191, batch running time: 0:22
train Batch:840, loss:0.07655, batch running time: 0:09
train Batch:854, loss:0.04764, batch running time: 0:55
train Batch:868, loss:0.14433, batch running time: 0:41
train Batch:882, loss:0.21285, batch running time: 0:27
train Batch:896, loss:0.05923, batch running time: 0:16
train Batch:910, loss:0.06441, batch running time: 0:02
train Batch:924, loss:0.14457, batch running time: 0:47
train Batch:938, loss:0.04841, batch running time: 0:36
train Batch:952, loss:0.08455, batch running time: 0:22
train Batch:966, loss:0.09093, batch running time: 0:08
train Batch:980, loss:0.14894, batch running time: 0:54
train Batch:994, loss:0.10716, batch running time: 0:35
train Batch:1008, loss:0.09206, batch running time: 0:19
train Batch:1022, loss:0.02758, batch running time: 0:07
train Batch:1036, loss:0.11482, batch running time: 0:47
train Batch:1050, loss:0.16488, batch running time: 0:35
train Batch:1064, loss:0.11781, batch running time: 0:19
train Batch:1078, loss:0.37198, batch running time: 0:05
train Batch:1092, loss:0.05054, batch running time: 0:53
train Batch:1106, loss:0.07514, batch running time: 0:41
train Batch:1120, loss:0.01201, batch running time: 0:32
train Batch:1134, loss:0.10649, batch running time: 0:17
train Batch:1148, loss:0.09801, batch running time: 0:03
train Batch:1162, loss:0.12149, batch running time: 0:47
train Batch:1176, loss:0.20652, batch running time: 0:31
train Batch:1190, loss:0.02325, batch running time: 0:19
train Batch:1204, loss:0.02775, batch running time: 0:07
train Batch:1218, loss:0.03207, batch running time: 0:55
train Batch:1232, loss:0.03315, batch running time: 0:45
train Batch:1246, loss:0.46920, batch running time: 0:31
train Batch:1260, loss:0.25768, batch running time: 0:15
train Batch:1274, loss:0.02289, batch running time: 0:02
train Batch:1288, loss:0.01757, batch running time: 0:50
train Batch:1302, loss:0.18912, batch running time: 0:33
train Batch:1316, loss:0.01171, batch running time: 0:20
train Batch:1330, loss:0.09385, batch running time: 0:04
train Batch:1344, loss:0.00634, batch running time: 0:44
train Batch:1358, loss:0.00715, batch running time: 0:32
--------------------------------------------------
train Epoch:24, Loss: 0.1342, lr: 0.00010000, Total running time: 36:42
Epoch: 25/1300
=======================================================
train Batch:14, loss:0.07251, batch running time: 0:55
train Batch:28, loss:0.15455, batch running time: 0:44
train Batch:42, loss:0.13209, batch running time: 0:30
train Batch:56, loss:0.04998, batch running time: 0:14
train Batch:70, loss:0.19884, batch running time: 0:01
train Batch:84, loss:0.44022, batch running time: 0:41
train Batch:98, loss:0.13464, batch running time: 0:29
train Batch:112, loss:0.54239, batch running time: 0:13
train Batch:126, loss:0.01816, batch running time: 0:59
train Batch:140, loss:0.01212, batch running time: 0:49
train Batch:154, loss:0.03973, batch running time: 0:32
train Batch:168, loss:0.03123, batch running time: 0:20
train Batch:182, loss:0.03682, batch running time: 0:06
train Batch:196, loss:0.30280, batch running time: 0:53
train Batch:210, loss:0.16500, batch running time: 0:42
train Batch:224, loss:0.18141, batch running time: 0:25
train Batch:238, loss:0.02460, batch running time: 0:13
train Batch:252, loss:0.03916, batch running time: 0:00
train Batch:266, loss:0.02236, batch running time: 0:45
train Batch:280, loss:0.93532, batch running time: 0:31
train Batch:294, loss:0.21786, batch running time: 0:16
train Batch:308, loss:0.02110, batch running time: 0:05
train Batch:322, loss:0.01008, batch running time: 0:50
train Batch:336, loss:0.01707, batch running time: 0:35
train Batch:350, loss:0.04676, batch running time: 0:21
train Batch:364, loss:0.00946, batch running time: 0:09
train Batch:378, loss:0.27767, batch running time: 0:50
train Batch:392, loss:0.15645, batch running time: 0:38
train Batch:406, loss:0.16266, batch running time: 0:24
train Batch:420, loss:0.01578, batch running time: 0:15
train Batch:434, loss:0.09183, batch running time: 0:02
train Batch:448, loss:0.29674, batch running time: 0:49
train Batch:462, loss:0.24380, batch running time: 0:30
train Batch:476, loss:0.01881, batch running time: 0:20
train Batch:490, loss:0.02213, batch running time: 0:06
train Batch:504, loss:0.14906, batch running time: 0:48
train Batch:518, loss:0.31304, batch running time: 0:30
train Batch:532, loss:0.02810, batch running time: 0:18
train Batch:546, loss:0.06184, batch running time: 0:04
train Batch:560, loss:0.40604, batch running time: 0:47
train Batch:574, loss:0.01938, batch running time: 0:32
train Batch:588, loss:0.03892, batch running time: 0:20
train Batch:602, loss:0.15732, batch running time: 0:05
train Batch:616, loss:0.09330, batch running time: 0:51
train Batch:630, loss:0.31367, batch running time: 0:32
train Batch:644, loss:0.21499, batch running time: 0:14
train Batch:658, loss:0.06332, batch running time: 0:58
train Batch:672, loss:0.45745, batch running time: 0:45
train Batch:686, loss:0.04327, batch running time: 0:28
train Batch:700, loss:0.42505, batch running time: 0:15
train Batch:714, loss:0.05833, batch running time: 0:04
train Batch:728, loss:0.00938, batch running time: 0:52
train Batch:742, loss:0.30748, batch running time: 0:33
train Batch:756, loss:0.04160, batch running time: 0:24
train Batch:770, loss:0.02114, batch running time: 0:10
train Batch:784, loss:0.26058, batch running time: 0:51
train Batch:798, loss:0.38739, batch running time: 0:40
train Batch:812, loss:0.13211, batch running time: 0:27
train Batch:826, loss:0.04507, batch running time: 0:10
train Batch:840, loss:0.01709, batch running time: 0:55
train Batch:854, loss:0.06554, batch running time: 0:47
train Batch:868, loss:0.11379, batch running time: 0:32
train Batch:882, loss:0.30932, batch running time: 0:12
train Batch:896, loss:0.01163, batch running time: 0:55
train Batch:910, loss:0.06306, batch running time: 0:46
train Batch:924, loss:0.34204, batch running time: 0:33
train Batch:938, loss:0.01150, batch running time: 0:19
train Batch:952, loss:0.10202, batch running time: 0:04
train Batch:966, loss:0.06921, batch running time: 0:49
train Batch:980, loss:0.03228, batch running time: 0:32
train Batch:994, loss:0.14194, batch running time: 0:19
train Batch:1008, loss:0.04752, batch running time: 0:08
train Batch:1022, loss:0.16348, batch running time: 0:53
train Batch:1036, loss:0.00672, batch running time: 0:41
train Batch:1050, loss:0.17164, batch running time: 0:25
train Batch:1064, loss:0.21612, batch running time: 0:06
train Batch:1078, loss:0.01355, batch running time: 0:55
train Batch:1092, loss:0.06607, batch running time: 0:43
train Batch:1106, loss:0.04786, batch running time: 0:28
train Batch:1120, loss:0.02749, batch running time: 0:16
train Batch:1134, loss:0.29025, batch running time: 0:03
train Batch:1148, loss:0.09855, batch running time: 0:51
train Batch:1162, loss:0.15228, batch running time: 0:34
train Batch:1176, loss:0.22603, batch running time: 0:19
train Batch:1190, loss:0.01821, batch running time: 0:05
train Batch:1204, loss:0.07704, batch running time: 0:52
train Batch:1218, loss:0.07976, batch running time: 0:43
train Batch:1232, loss:0.07568, batch running time: 0:31
train Batch:1246, loss:0.08437, batch running time: 0:16
train Batch:1260, loss:0.04801, batch running time: 0:07
train Batch:1274, loss:0.24041, batch running time: 0:50
train Batch:1288, loss:0.28876, batch running time: 0:36
train Batch:1302, loss:0.00746, batch running time: 0:25
train Batch:1316, loss:0.01992, batch running time: 0:10
train Batch:1330, loss:0.23246, batch running time: 0:56
train Batch:1344, loss:0.03647, batch running time: 0:45
train Batch:1358, loss:0.14290, batch running time: 0:34
Epoch    24: reducing learning rate of group 0 to 1.0000e-05.
--------------------------------------------------
train Epoch:25, Loss: 0.1319, lr: 0.00001000, Total running time: 37:11
Epoch: 26/1300
=======================================================
train Batch:14, loss:0.07301, batch running time: 0:53
train Batch:28, loss:0.08764, batch running time: 0:39
train Batch:42, loss:0.60966, batch running time: 0:22
train Batch:56, loss:0.15387, batch running time: 0:10
train Batch:70, loss:0.04232, batch running time: 0:53
train Batch:84, loss:0.09638, batch running time: 0:38
train Batch:98, loss:0.02933, batch running time: 0:27
train Batch:112, loss:0.05104, batch running time: 0:14
train Batch:126, loss:0.00384, batch running time: 0:58
train Batch:140, loss:0.08622, batch running time: 0:42
train Batch:154, loss:0.26515, batch running time: 0:28
train Batch:168, loss:0.09922, batch running time: 0:16
train Batch:182, loss:0.01774, batch running time: 0:01
train Batch:196, loss:0.32156, batch running time: 0:46
train Batch:210, loss:0.04304, batch running time: 0:33
train Batch:224, loss:0.00532, batch running time: 0:20
train Batch:238, loss:0.03067, batch running time: 0:07
train Batch:252, loss:0.02875, batch running time: 0:54
train Batch:266, loss:0.05410, batch running time: 0:39
train Batch:280, loss:0.15905, batch running time: 0:24
train Batch:294, loss:0.09343, batch running time: 0:14
train Batch:308, loss:0.26461, batch running time: 0:59
train Batch:322, loss:0.15055, batch running time: 0:49
train Batch:336, loss:0.07055, batch running time: 0:34
train Batch:350, loss:0.03928, batch running time: 0:23
train Batch:364, loss:0.32745, batch running time: 0:08
train Batch:378, loss:0.09960, batch running time: 0:57
train Batch:392, loss:0.43335, batch running time: 0:40
train Batch:406, loss:0.15629, batch running time: 0:27
train Batch:420, loss:0.01977, batch running time: 0:10
train Batch:434, loss:0.02315, batch running time: 0:58
train Batch:448, loss:0.05476, batch running time: 0:41
train Batch:462, loss:0.19921, batch running time: 0:28
train Batch:476, loss:0.01621, batch running time: 0:14
train Batch:490, loss:0.12786, batch running time: 0:01
train Batch:504, loss:0.00820, batch running time: 0:51
train Batch:518, loss:0.28280, batch running time: 0:36
train Batch:532, loss:0.03458, batch running time: 0:24
train Batch:546, loss:0.00099, batch running time: 0:11
train Batch:560, loss:0.24264, batch running time: 0:57
train Batch:574, loss:0.52798, batch running time: 0:44
train Batch:588, loss:0.04854, batch running time: 0:30
train Batch:602, loss:0.45590, batch running time: 0:13
train Batch:616, loss:0.00716, batch running time: 0:01
train Batch:630, loss:0.57659, batch running time: 0:49
train Batch:644, loss:0.00517, batch running time: 0:31
train Batch:658, loss:0.02549, batch running time: 0:20
train Batch:672, loss:0.14866, batch running time: 0:01
train Batch:686, loss:0.16222, batch running time: 0:49
train Batch:700, loss:0.26505, batch running time: 0:29
train Batch:714, loss:0.25044, batch running time: 0:13
train Batch:728, loss:0.29887, batch running time: 0:02
train Batch:742, loss:0.00942, batch running time: 0:50
train Batch:756, loss:0.02038, batch running time: 0:39
train Batch:770, loss:0.01606, batch running time: 0:24
train Batch:784, loss:0.26277, batch running time: 0:08
train Batch:798, loss:0.01283, batch running time: 0:55
train Batch:812, loss:0.06942, batch running time: 0:44
train Batch:826, loss:0.01001, batch running time: 0:29
train Batch:840, loss:0.39127, batch running time: 0:14
train Batch:854, loss:0.26641, batch running time: 0:01
train Batch:868, loss:0.09434, batch running time: 0:47
train Batch:882, loss:0.05347, batch running time: 0:31
train Batch:896, loss:0.01137, batch running time: 0:17
train Batch:910, loss:0.61979, batch running time: 0:02
train Batch:924, loss:0.00992, batch running time: 0:51
train Batch:938, loss:0.14944, batch running time: 0:39
train Batch:952, loss:0.10146, batch running time: 0:28
train Batch:966, loss:0.08640, batch running time: 0:14
train Batch:980, loss:0.03008, batch running time: 0:59
train Batch:994, loss:0.07361, batch running time: 0:45
train Batch:1008, loss:0.20500, batch running time: 0:33
train Batch:1022, loss:0.03182, batch running time: 0:22
train Batch:1036, loss:0.02634, batch running time: 0:09
train Batch:1050, loss:0.46487, batch running time: 0:56
train Batch:1064, loss:0.36597, batch running time: 0:44
train Batch:1078, loss:0.01030, batch running time: 0:30
train Batch:1092, loss:0.00448, batch running time: 0:17
train Batch:1106, loss:0.00548, batch running time: 0:04
train Batch:1120, loss:0.29406, batch running time: 0:52
train Batch:1134, loss:0.08960, batch running time: 0:40
train Batch:1148, loss:0.03838, batch running time: 0:28
train Batch:1162, loss:0.00527, batch running time: 0:18
train Batch:1176, loss:0.20932, batch running time: 0:01
train Batch:1190, loss:0.74528, batch running time: 0:42
train Batch:1204, loss:0.01207, batch running time: 0:25
train Batch:1218, loss:0.02390, batch running time: 0:11
train Batch:1232, loss:0.06303, batch running time: 0:59
train Batch:1246, loss:0.06511, batch running time: 0:48
train Batch:1260, loss:0.10777, batch running time: 0:35
train Batch:1274, loss:0.03647, batch running time: 0:23
train Batch:1288, loss:0.02866, batch running time: 0:08
train Batch:1302, loss:0.18427, batch running time: 0:56
train Batch:1316, loss:0.03036, batch running time: 0:46
train Batch:1330, loss:0.16209, batch running time: 0:34
train Batch:1344, loss:0.02047, batch running time: 0:22
train Batch:1358, loss:0.01715, batch running time: 0:09
Lowest loss so far:0.12900995797100848, snapshotting weights to BestWeight-foodNonfood-res50-02-08-2019_17-07-57.pth
--------------------------------------------------
train Epoch:26, Loss: 0.1290, lr: 0.00001000, Total running time: 39:40
Epoch: 27/1300
=======================================================
train Batch:14, loss:0.30675, batch running time: 0:53
train Batch:28, loss:0.11639, batch running time: 0:41
train Batch:42, loss:0.13608, batch running time: 0:28
train Batch:56, loss:0.05132, batch running time: 0:11
train Batch:70, loss:0.11376, batch running time: 0:55
train Batch:84, loss:0.03026, batch running time: 0:41
train Batch:98, loss:0.07253, batch running time: 0:26
train Batch:112, loss:0.08763, batch running time: 0:17
train Batch:126, loss:0.64235, batch running time: 0:57
train Batch:140, loss:0.09444, batch running time: 0:43
train Batch:154, loss:0.37437, batch running time: 0:29
train Batch:168, loss:0.37321, batch running time: 0:11
train Batch:182, loss:0.00634, batch running time: 0:58
train Batch:196, loss:0.01509, batch running time: 0:43
train Batch:210, loss:0.19270, batch running time: 0:30
train Batch:224, loss:0.10078, batch running time: 0:21
train Batch:238, loss:0.02820, batch running time: 0:09
train Batch:252, loss:0.15981, batch running time: 0:55
train Batch:266, loss:0.07365, batch running time: 0:43
train Batch:280, loss:0.35711, batch running time: 0:29
train Batch:294, loss:0.14158, batch running time: 0:18
train Batch:308, loss:0.03859, batch running time: 0:06
train Batch:322, loss:0.08197, batch running time: 0:51
train Batch:336, loss:0.39822, batch running time: 0:36
train Batch:350, loss:0.11832, batch running time: 0:24
train Batch:364, loss:0.03042, batch running time: 0:08
train Batch:378, loss:0.05161, batch running time: 0:52
train Batch:392, loss:0.20950, batch running time: 0:37
train Batch:406, loss:0.04167, batch running time: 0:20
train Batch:420, loss:0.19922, batch running time: 0:08
train Batch:434, loss:0.00787, batch running time: 0:57
train Batch:448, loss:0.01268, batch running time: 0:41
train Batch:462, loss:0.09146, batch running time: 0:29
train Batch:476, loss:0.00844, batch running time: 0:14
train Batch:490, loss:0.35927, batch running time: 0:58
train Batch:504, loss:0.19956, batch running time: 0:46
train Batch:518, loss:0.03382, batch running time: 0:33
train Batch:532, loss:0.56637, batch running time: 0:17
train Batch:546, loss:0.05453, batch running time: 0:06
train Batch:560, loss:0.06785, batch running time: 0:55
train Batch:574, loss:0.10573, batch running time: 0:42
train Batch:588, loss:0.08314, batch running time: 0:25
train Batch:602, loss:0.00581, batch running time: 0:15
train Batch:616, loss:0.03083, batch running time: 0:59
train Batch:630, loss:0.05773, batch running time: 0:46
train Batch:644, loss:0.05190, batch running time: 0:29
train Batch:658, loss:0.04917, batch running time: 0:12
train Batch:672, loss:0.13497, batch running time: 0:55
train Batch:686, loss:0.01606, batch running time: 0:43
train Batch:700, loss:0.06543, batch running time: 0:28
train Batch:714, loss:0.12222, batch running time: 0:12
train Batch:728, loss:0.34926, batch running time: 0:57
train Batch:742, loss:0.00757, batch running time: 0:43
train Batch:756, loss:0.04711, batch running time: 0:30
train Batch:770, loss:0.65953, batch running time: 0:18
train Batch:784, loss:0.01029, batch running time: 0:04
train Batch:798, loss:0.07598, batch running time: 0:55
train Batch:812, loss:0.03639, batch running time: 0:45
train Batch:826, loss:0.06971, batch running time: 0:30
train Batch:840, loss:0.01037, batch running time: 0:14
train Batch:854, loss:0.02913, batch running time: 0:02
train Batch:868, loss:0.35418, batch running time: 0:47
train Batch:882, loss:0.13679, batch running time: 0:32
train Batch:896, loss:0.02763, batch running time: 0:21
train Batch:910, loss:0.06646, batch running time: 0:10
train Batch:924, loss:0.21446, batch running time: 0:56
train Batch:938, loss:0.02116, batch running time: 0:43
train Batch:952, loss:0.03546, batch running time: 0:28
train Batch:966, loss:0.12834, batch running time: 0:13
train Batch:980, loss:0.12137, batch running time: 0:57
train Batch:994, loss:0.01964, batch running time: 0:45
train Batch:1008, loss:0.12756, batch running time: 0:32
train Batch:1022, loss:0.04383, batch running time: 0:19
train Batch:1036, loss:0.10980, batch running time: 0:06
train Batch:1050, loss:0.08415, batch running time: 0:54
train Batch:1064, loss:0.01115, batch running time: 0:40
train Batch:1078, loss:0.09218, batch running time: 0:25
train Batch:1092, loss:0.40670, batch running time: 0:11
train Batch:1106, loss:0.02306, batch running time: 0:56
train Batch:1120, loss:0.25908, batch running time: 0:39
train Batch:1134, loss:0.37118, batch running time: 0:25
train Batch:1148, loss:0.11438, batch running time: 0:14
train Batch:1162, loss:0.13000, batch running time: 0:01
train Batch:1176, loss:0.01919, batch running time: 0:47
train Batch:1190, loss:0.03658, batch running time: 0:37
train Batch:1204, loss:0.24151, batch running time: 0:22
train Batch:1218, loss:0.11166, batch running time: 0:04
train Batch:1232, loss:0.03096, batch running time: 0:52
train Batch:1246, loss:0.10262, batch running time: 0:34
train Batch:1260, loss:0.15083, batch running time: 0:22
train Batch:1274, loss:0.05712, batch running time: 0:11
train Batch:1288, loss:0.07096, batch running time: 0:57
train Batch:1302, loss:0.03889, batch running time: 0:43
train Batch:1316, loss:0.13840, batch running time: 0:28
train Batch:1330, loss:0.21572, batch running time: 0:11
train Batch:1344, loss:0.00333, batch running time: 0:00
train Batch:1358, loss:0.09736, batch running time: 0:46
--------------------------------------------------
train Epoch:27, Loss: 0.1294, lr: 0.00001000, Total running time: 40:10
Epoch: 28/1300
=======================================================
train Batch:14, loss:0.06844, batch running time: 0:58
train Batch:28, loss:0.01401, batch running time: 0:45
train Batch:42, loss:0.03066, batch running time: 0:30
train Batch:56, loss:0.20810, batch running time: 0:15
train Batch:70, loss:0.02351, batch running time: 0:00
train Batch:84, loss:0.05859, batch running time: 0:49
train Batch:98, loss:0.18888, batch running time: 0:35
train Batch:112, loss:0.17212, batch running time: 0:24
train Batch:126, loss:0.01707, batch running time: 0:06
train Batch:140, loss:0.05390, batch running time: 0:54
train Batch:154, loss:0.03189, batch running time: 0:44
train Batch:168, loss:0.03383, batch running time: 0:31
train Batch:182, loss:0.16575, batch running time: 0:16
train Batch:196, loss:0.02438, batch running time: 0:02
train Batch:210, loss:0.11752, batch running time: 0:50
train Batch:224, loss:0.04794, batch running time: 0:36
train Batch:238, loss:0.00824, batch running time: 0:24
train Batch:252, loss:0.12919, batch running time: 0:08
train Batch:266, loss:0.11599, batch running time: 0:57
train Batch:280, loss:0.62086, batch running time: 0:41
train Batch:294, loss:0.06437, batch running time: 0:27
train Batch:308, loss:0.38358, batch running time: 0:14
train Batch:322, loss:0.66144, batch running time: 0:59
train Batch:336, loss:0.56394, batch running time: 0:44
train Batch:350, loss:0.01200, batch running time: 0:30
train Batch:364, loss:0.02006, batch running time: 0:12
train Batch:378, loss:0.30969, batch running time: 0:58
train Batch:392, loss:0.09483, batch running time: 0:42
train Batch:406, loss:0.12136, batch running time: 0:32
train Batch:420, loss:0.02894, batch running time: 0:15
train Batch:434, loss:0.04542, batch running time: 0:06
train Batch:448, loss:0.49525, batch running time: 0:47
train Batch:462, loss:0.06220, batch running time: 0:36
train Batch:476, loss:0.01230, batch running time: 0:26
train Batch:490, loss:0.05403, batch running time: 0:17
train Batch:504, loss:0.14789, batch running time: 0:00
train Batch:518, loss:0.32388, batch running time: 0:45
train Batch:532, loss:0.01569, batch running time: 0:33
train Batch:546, loss:0.01157, batch running time: 0:15
train Batch:560, loss:0.01441, batch running time: 0:02
train Batch:574, loss:0.05340, batch running time: 0:50
train Batch:588, loss:0.04106, batch running time: 0:38
train Batch:602, loss:0.13397, batch running time: 0:25
train Batch:616, loss:0.12318, batch running time: 0:11
train Batch:630, loss:0.03527, batch running time: 0:01
train Batch:644, loss:0.26553, batch running time: 0:45
train Batch:658, loss:0.01701, batch running time: 0:36
train Batch:672, loss:0.01076, batch running time: 0:23
train Batch:686, loss:0.12849, batch running time: 0:13
train Batch:700, loss:0.01187, batch running time: 0:59
train Batch:714, loss:0.12518, batch running time: 0:46
train Batch:728, loss:0.26015, batch running time: 0:32
train Batch:742, loss:0.12226, batch running time: 0:19
train Batch:756, loss:0.26100, batch running time: 0:07
train Batch:770, loss:0.09975, batch running time: 0:52
train Batch:784, loss:0.13196, batch running time: 0:41
train Batch:798, loss:0.05746, batch running time: 0:25
train Batch:812, loss:0.06463, batch running time: 0:15
train Batch:826, loss:0.02891, batch running time: 0:01
train Batch:840, loss:0.15689, batch running time: 0:45
train Batch:854, loss:0.01672, batch running time: 0:32
train Batch:868, loss:0.00779, batch running time: 0:16
train Batch:882, loss:0.30115, batch running time: 0:04
train Batch:896, loss:0.02674, batch running time: 0:49
train Batch:910, loss:0.21884, batch running time: 0:38
train Batch:924, loss:0.00966, batch running time: 0:21
train Batch:938, loss:0.01188, batch running time: 0:09
train Batch:952, loss:0.01218, batch running time: 0:56
train Batch:966, loss:0.00308, batch running time: 0:44
train Batch:980, loss:0.02882, batch running time: 0:34
train Batch:994, loss:0.15691, batch running time: 0:24
train Batch:1008, loss:0.07696, batch running time: 0:13
train Batch:1022, loss:0.02563, batch running time: 0:00
train Batch:1036, loss:0.03995, batch running time: 0:50
train Batch:1050, loss:0.26120, batch running time: 0:35
train Batch:1064, loss:0.00315, batch running time: 0:22
train Batch:1078, loss:0.07022, batch running time: 0:08
train Batch:1092, loss:0.10618, batch running time: 0:55
train Batch:1106, loss:0.00427, batch running time: 0:41
train Batch:1120, loss:0.19765, batch running time: 0:28
train Batch:1134, loss:0.03609, batch running time: 0:15
train Batch:1148, loss:0.01979, batch running time: 0:59
train Batch:1162, loss:0.06798, batch running time: 0:46
train Batch:1176, loss:0.01209, batch running time: 0:35
train Batch:1190, loss:0.22815, batch running time: 0:24
train Batch:1204, loss:0.00501, batch running time: 0:11
train Batch:1218, loss:0.08149, batch running time: 0:01
train Batch:1232, loss:0.12528, batch running time: 0:49
train Batch:1246, loss:0.03765, batch running time: 0:36
train Batch:1260, loss:0.13445, batch running time: 0:25
train Batch:1274, loss:0.05013, batch running time: 0:15
train Batch:1288, loss:0.31137, batch running time: 0:58
train Batch:1302, loss:0.14716, batch running time: 0:48
train Batch:1316, loss:0.09888, batch running time: 0:31
train Batch:1330, loss:0.03517, batch running time: 0:18
train Batch:1344, loss:0.14035, batch running time: 0:04
train Batch:1358, loss:0.06262, batch running time: 0:48
Lowest loss so far:0.12439105454861347, snapshotting weights to BestWeight-foodNonfood-res50-02-08-2019_17-07-57.pth
--------------------------------------------------
train Epoch:28, Loss: 0.1244, lr: 0.00001000, Total running time: 42:40
Epoch: 29/1300
=======================================================
train Batch:14, loss:0.14083, batch running time: 0:56
train Batch:28, loss:0.02690, batch running time: 0:41
train Batch:42, loss:0.01436, batch running time: 0:31
train Batch:56, loss:0.08857, batch running time: 0:16
train Batch:70, loss:0.18439, batch running time: 0:02
train Batch:84, loss:0.23701, batch running time: 0:52
train Batch:98, loss:0.13536, batch running time: 0:39
train Batch:112, loss:0.05384, batch running time: 0:21
train Batch:126, loss:0.00756, batch running time: 0:08
train Batch:140, loss:0.22098, batch running time: 0:55
train Batch:154, loss:0.55683, batch running time: 0:42
train Batch:168, loss:0.18879, batch running time: 0:27
train Batch:182, loss:0.00557, batch running time: 0:09
train Batch:196, loss:0.21661, batch running time: 0:54
train Batch:210, loss:0.22450, batch running time: 0:43
train Batch:224, loss:0.76920, batch running time: 0:27
train Batch:238, loss:0.08068, batch running time: 0:13
train Batch:252, loss:0.16718, batch running time: 0:01
train Batch:266, loss:0.18078, batch running time: 0:46
train Batch:280, loss:0.01740, batch running time: 0:27
train Batch:294, loss:0.04639, batch running time: 0:18
train Batch:308, loss:0.05280, batch running time: 0:06
train Batch:322, loss:0.01632, batch running time: 0:53
train Batch:336, loss:0.00522, batch running time: 0:38
train Batch:350, loss:0.08278, batch running time: 0:26
train Batch:364, loss:0.07517, batch running time: 0:13
train Batch:378, loss:0.05352, batch running time: 0:00
train Batch:392, loss:0.02123, batch running time: 0:51
train Batch:406, loss:0.25748, batch running time: 0:36
train Batch:420, loss:0.04055, batch running time: 0:23
train Batch:434, loss:0.08738, batch running time: 0:10
train Batch:448, loss:0.10288, batch running time: 0:58
train Batch:462, loss:0.02605, batch running time: 0:44
train Batch:476, loss:0.12570, batch running time: 0:29
train Batch:490, loss:0.09505, batch running time: 0:14
train Batch:504, loss:0.08551, batch running time: 0:02
train Batch:518, loss:0.02017, batch running time: 0:50
train Batch:532, loss:0.17098, batch running time: 0:36
train Batch:546, loss:0.03195, batch running time: 0:19
train Batch:560, loss:0.03982, batch running time: 0:06
train Batch:574, loss:0.02134, batch running time: 0:52
train Batch:588, loss:0.04536, batch running time: 0:37
train Batch:602, loss:0.41411, batch running time: 0:24
train Batch:616, loss:0.00823, batch running time: 0:13
train Batch:630, loss:0.01711, batch running time: 0:02
train Batch:644, loss:0.74610, batch running time: 0:48
train Batch:658, loss:0.05530, batch running time: 0:37
train Batch:672, loss:0.01647, batch running time: 0:28
train Batch:686, loss:0.03778, batch running time: 0:16
train Batch:700, loss:0.10890, batch running time: 0:04
train Batch:714, loss:0.11231, batch running time: 0:51
train Batch:728, loss:0.01212, batch running time: 0:35
train Batch:742, loss:0.54860, batch running time: 0:19
train Batch:756, loss:0.07079, batch running time: 0:06
train Batch:770, loss:0.02239, batch running time: 0:55
train Batch:784, loss:0.01177, batch running time: 0:40
train Batch:798, loss:0.13869, batch running time: 0:30
train Batch:812, loss:0.16228, batch running time: 0:20
train Batch:826, loss:0.07040, batch running time: 0:05
train Batch:840, loss:0.01829, batch running time: 0:47
train Batch:854, loss:0.22549, batch running time: 0:33
train Batch:868, loss:0.06010, batch running time: 0:20
train Batch:882, loss:0.25627, batch running time: 0:03
train Batch:896, loss:0.31019, batch running time: 0:48
train Batch:910, loss:0.32350, batch running time: 0:33
train Batch:924, loss:0.20989, batch running time: 0:19
train Batch:938, loss:0.00494, batch running time: 0:04
train Batch:952, loss:0.02209, batch running time: 0:51
train Batch:966, loss:0.03058, batch running time: 0:39
train Batch:980, loss:0.18869, batch running time: 0:28
train Batch:994, loss:0.13655, batch running time: 0:15
train Batch:1008, loss:0.01110, batch running time: 0:03
train Batch:1022, loss:0.06277, batch running time: 0:50
train Batch:1036, loss:0.00717, batch running time: 0:41
train Batch:1050, loss:0.29873, batch running time: 0:27
train Batch:1064, loss:0.00730, batch running time: 0:14
train Batch:1078, loss:0.00748, batch running time: 0:59
train Batch:1092, loss:0.12862, batch running time: 0:47
train Batch:1106, loss:0.01623, batch running time: 0:36
train Batch:1120, loss:0.12158, batch running time: 0:25
train Batch:1134, loss:0.03165, batch running time: 0:13
train Batch:1148, loss:0.01040, batch running time: 0:59
train Batch:1162, loss:0.03860, batch running time: 0:43
train Batch:1176, loss:0.01067, batch running time: 0:27
train Batch:1190, loss:0.03537, batch running time: 0:12
train Batch:1204, loss:0.08480, batch running time: 0:57
train Batch:1218, loss:0.43446, batch running time: 0:41
train Batch:1232, loss:0.00828, batch running time: 0:28
train Batch:1246, loss:0.09753, batch running time: 0:17
train Batch:1260, loss:0.00907, batch running time: 0:05
train Batch:1274, loss:0.07764, batch running time: 0:52
train Batch:1288, loss:0.14973, batch running time: 0:36
train Batch:1302, loss:0.00160, batch running time: 0:22
train Batch:1316, loss:0.08397, batch running time: 0:08
train Batch:1330, loss:0.05006, batch running time: 0:54
train Batch:1344, loss:0.50968, batch running time: 0:41
train Batch:1358, loss:0.00165, batch running time: 0:29
--------------------------------------------------
train Epoch:29, Loss: 0.1256, lr: 0.00001000, Total running time: 43:10
Epoch: 30/1300
=======================================================
train Batch:14, loss:0.07967, batch running time: 0:50
train Batch:28, loss:0.02453, batch running time: 0:36
train Batch:42, loss:0.47390, batch running time: 0:25
train Batch:56, loss:0.02413, batch running time: 0:16
train Batch:70, loss:0.07626, batch running time: 0:01
train Batch:84, loss:0.10300, batch running time: 0:48
train Batch:98, loss:0.04336, batch running time: 0:33
train Batch:112, loss:0.01393, batch running time: 0:18
train Batch:126, loss:0.09771, batch running time: 0:04
train Batch:140, loss:0.00866, batch running time: 0:54
train Batch:154, loss:0.07756, batch running time: 0:43
train Batch:168, loss:0.10417, batch running time: 0:31
train Batch:182, loss:0.21896, batch running time: 0:16
train Batch:196, loss:0.46633, batch running time: 0:04
train Batch:210, loss:0.03216, batch running time: 0:52
train Batch:224, loss:0.00646, batch running time: 0:39
train Batch:238, loss:0.08183, batch running time: 0:27
train Batch:252, loss:0.07673, batch running time: 0:07
train Batch:266, loss:0.00843, batch running time: 0:55
train Batch:280, loss:0.03490, batch running time: 0:43
train Batch:294, loss:0.03375, batch running time: 0:31
train Batch:308, loss:0.63730, batch running time: 0:14
train Batch:322, loss:0.14275, batch running time: 0:00
train Batch:336, loss:0.20084, batch running time: 0:47
train Batch:350, loss:0.02747, batch running time: 0:35
train Batch:364, loss:0.15532, batch running time: 0:25
train Batch:378, loss:0.38218, batch running time: 0:11
train Batch:392, loss:0.01793, batch running time: 0:57
train Batch:406, loss:0.16105, batch running time: 0:42
train Batch:420, loss:0.02666, batch running time: 0:30
train Batch:434, loss:0.01124, batch running time: 0:17
train Batch:448, loss:0.35957, batch running time: 0:02
train Batch:462, loss:0.31417, batch running time: 0:47
train Batch:476, loss:0.25008, batch running time: 0:31
train Batch:490, loss:0.38585, batch running time: 0:15
train Batch:504, loss:0.15130, batch running time: 0:02
train Batch:518, loss:0.18965, batch running time: 0:48
train Batch:532, loss:0.05860, batch running time: 0:38
train Batch:546, loss:0.09464, batch running time: 0:28
train Batch:560, loss:0.02162, batch running time: 0:13
train Batch:574, loss:0.05837, batch running time: 0:03
train Batch:588, loss:0.08709, batch running time: 0:48
train Batch:602, loss:0.01136, batch running time: 0:36
train Batch:616, loss:0.02949, batch running time: 0:24
train Batch:630, loss:0.10219, batch running time: 0:13
train Batch:644, loss:0.10626, batch running time: 0:58
train Batch:658, loss:0.12668, batch running time: 0:48
train Batch:672, loss:0.09774, batch running time: 0:33
train Batch:686, loss:0.03432, batch running time: 0:20
train Batch:700, loss:0.01772, batch running time: 0:03
train Batch:714, loss:0.02009, batch running time: 0:49
train Batch:728, loss:0.48859, batch running time: 0:30
train Batch:742, loss:0.34725, batch running time: 0:15
train Batch:756, loss:0.08865, batch running time: 0:59
train Batch:770, loss:1.35351, batch running time: 0:41
train Batch:784, loss:0.00996, batch running time: 0:25
train Batch:798, loss:0.02692, batch running time: 0:09
train Batch:812, loss:0.15925, batch running time: 0:55
train Batch:826, loss:0.49695, batch running time: 0:35
train Batch:840, loss:0.13133, batch running time: 0:20
train Batch:854, loss:0.07945, batch running time: 0:04
train Batch:868, loss:0.03486, batch running time: 0:51
train Batch:882, loss:0.18443, batch running time: 0:40
train Batch:896, loss:0.33710, batch running time: 0:26
train Batch:910, loss:0.32353, batch running time: 0:07
train Batch:924, loss:0.01463, batch running time: 0:56
train Batch:938, loss:0.01847, batch running time: 0:41
train Batch:952, loss:0.16726, batch running time: 0:30
train Batch:966, loss:0.32553, batch running time: 0:17
train Batch:980, loss:0.09433, batch running time: 0:07
train Batch:994, loss:0.01360, batch running time: 0:53
train Batch:1008, loss:0.00371, batch running time: 0:41
train Batch:1022, loss:0.13254, batch running time: 0:25
train Batch:1036, loss:0.34170, batch running time: 0:08
train Batch:1050, loss:0.06221, batch running time: 0:53
train Batch:1064, loss:0.07243, batch running time: 0:38
train Batch:1078, loss:0.02738, batch running time: 0:27
train Batch:1092, loss:0.02141, batch running time: 0:14
train Batch:1106, loss:0.25770, batch running time: 0:00
train Batch:1120, loss:0.24525, batch running time: 0:47
train Batch:1134, loss:0.08856, batch running time: 0:33
train Batch:1148, loss:0.05665, batch running time: 0:23
train Batch:1162, loss:0.26020, batch running time: 0:11
train Batch:1176, loss:0.10157, batch running time: 0:56
train Batch:1190, loss:0.20468, batch running time: 0:43
train Batch:1204, loss:0.17935, batch running time: 0:31
train Batch:1218, loss:0.01354, batch running time: 0:17
train Batch:1232, loss:0.00932, batch running time: 0:07
train Batch:1246, loss:0.02073, batch running time: 0:53
train Batch:1260, loss:0.02039, batch running time: 0:42
train Batch:1274, loss:0.96965, batch running time: 0:28
train Batch:1288, loss:0.40761, batch running time: 0:17
train Batch:1302, loss:0.21985, batch running time: 0:02
train Batch:1316, loss:0.02760, batch running time: 0:51
train Batch:1330, loss:0.01490, batch running time: 0:39
train Batch:1344, loss:0.02235, batch running time: 0:29
train Batch:1358, loss:0.06161, batch running time: 0:16
Epoch    29: reducing learning rate of group 0 to 1.0000e-06.
--------------------------------------------------
train Epoch:30, Loss: 0.1259, lr: 0.00000100, Total running time: 45:40
Epoch: 31/1300
=======================================================
train Batch:14, loss:0.44851, batch running time: 0:51
train Batch:28, loss:0.08338, batch running time: 0:38
train Batch:42, loss:0.04835, batch running time: 0:24
train Batch:56, loss:0.20968, batch running time: 0:10
train Batch:70, loss:0.02341, batch running time: 0:52
train Batch:84, loss:0.01085, batch running time: 0:37
train Batch:98, loss:0.15652, batch running time: 0:22
train Batch:112, loss:0.05570, batch running time: 0:08
train Batch:126, loss:0.27710, batch running time: 0:57
train Batch:140, loss:0.04614, batch running time: 0:45
train Batch:154, loss:0.22041, batch running time: 0:32
train Batch:168, loss:0.03635, batch running time: 0:21
train Batch:182, loss:0.29532, batch running time: 0:08
train Batch:196, loss:0.03582, batch running time: 0:56
train Batch:210, loss:0.12300, batch running time: 0:41
train Batch:224, loss:0.17281, batch running time: 0:28
train Batch:238, loss:0.07069, batch running time: 0:11
train Batch:252, loss:0.09222, batch running time: 0:59
train Batch:266, loss:0.01074, batch running time: 0:47
train Batch:280, loss:0.03412, batch running time: 0:35
train Batch:294, loss:0.01480, batch running time: 0:19
train Batch:308, loss:0.15647, batch running time: 0:09
train Batch:322, loss:0.01012, batch running time: 0:58
train Batch:336, loss:0.08366, batch running time: 0:45
train Batch:350, loss:0.40506, batch running time: 0:36
train Batch:364, loss:0.06179, batch running time: 0:19
train Batch:378, loss:0.02825, batch running time: 0:00
train Batch:392, loss:0.12048, batch running time: 0:48
train Batch:406, loss:0.00352, batch running time: 0:36
train Batch:420, loss:0.00286, batch running time: 0:25
train Batch:434, loss:0.11397, batch running time: 0:11
train Batch:448, loss:0.03575, batch running time: 0:59
train Batch:462, loss:0.20804, batch running time: 0:46
train Batch:476, loss:0.13299, batch running time: 0:32
train Batch:490, loss:0.20308, batch running time: 0:21
train Batch:504, loss:0.02127, batch running time: 0:10
train Batch:518, loss:0.01711, batch running time: 0:55
train Batch:532, loss:0.00472, batch running time: 0:40
train Batch:546, loss:0.05067, batch running time: 0:26
train Batch:560, loss:0.37030, batch running time: 0:15
train Batch:574, loss:0.53707, batch running time: 0:00
train Batch:588, loss:0.20657, batch running time: 0:46
train Batch:602, loss:0.06957, batch running time: 0:35
train Batch:616, loss:0.08193, batch running time: 0:23
train Batch:630, loss:0.13569, batch running time: 0:08
train Batch:644, loss:0.02713, batch running time: 0:50
train Batch:658, loss:0.17721, batch running time: 0:33
train Batch:672, loss:0.04709, batch running time: 0:20
train Batch:686, loss:0.05204, batch running time: 0:09
train Batch:700, loss:0.14331, batch running time: 0:57
train Batch:714, loss:0.01204, batch running time: 0:44
train Batch:728, loss:0.34380, batch running time: 0:30
train Batch:742, loss:0.12749, batch running time: 0:14
train Batch:756, loss:0.04977, batch running time: 0:58
train Batch:770, loss:0.01471, batch running time: 0:45
train Batch:784, loss:0.30195, batch running time: 0:34
train Batch:798, loss:0.29258, batch running time: 0:21
train Batch:812, loss:0.00961, batch running time: 0:10
train Batch:826, loss:0.02409, batch running time: 0:58
train Batch:840, loss:0.25492, batch running time: 0:44
train Batch:854, loss:0.01109, batch running time: 0:33
train Batch:868, loss:0.03169, batch running time: 0:21
train Batch:882, loss:0.01389, batch running time: 0:08
train Batch:896, loss:0.05083, batch running time: 0:58
train Batch:910, loss:0.02744, batch running time: 0:46
train Batch:924, loss:0.01110, batch running time: 0:35
train Batch:938, loss:0.07761, batch running time: 0:22
train Batch:952, loss:0.15740, batch running time: 0:09
train Batch:966, loss:0.10489, batch running time: 0:57
train Batch:980, loss:0.20349, batch running time: 0:44
train Batch:994, loss:0.32648, batch running time: 0:29
train Batch:1008, loss:0.43982, batch running time: 0:12
train Batch:1022, loss:0.41931, batch running time: 0:58
train Batch:1036, loss:0.00768, batch running time: 0:41
train Batch:1050, loss:0.01638, batch running time: 0:30
train Batch:1064, loss:0.01685, batch running time: 0:20
train Batch:1078, loss:0.02316, batch running time: 0:10
train Batch:1092, loss:0.04268, batch running time: 0:56
train Batch:1106, loss:0.07211, batch running time: 0:45
train Batch:1120, loss:0.11781, batch running time: 0:31
train Batch:1134, loss:0.00986, batch running time: 0:19
train Batch:1148, loss:0.52592, batch running time: 0:04
train Batch:1162, loss:0.41642, batch running time: 0:48
train Batch:1176, loss:0.24603, batch running time: 0:34
train Batch:1190, loss:0.39023, batch running time: 0:19
train Batch:1204, loss:0.00549, batch running time: 0:07
train Batch:1218, loss:0.02042, batch running time: 0:57
train Batch:1232, loss:0.70478, batch running time: 0:42
train Batch:1246, loss:0.25645, batch running time: 0:26
train Batch:1260, loss:0.20834, batch running time: 0:16
train Batch:1274, loss:0.04941, batch running time: 0:01
train Batch:1288, loss:0.22467, batch running time: 0:49
train Batch:1302, loss:0.00845, batch running time: 0:35
train Batch:1316, loss:0.44896, batch running time: 0:21
train Batch:1330, loss:0.26524, batch running time: 0:08
train Batch:1344, loss:0.25823, batch running time: 0:57
train Batch:1358, loss:0.00640, batch running time: 0:44
Lowest loss so far:0.12334468146505852, snapshotting weights to BestWeight-foodNonfood-res50-02-08-2019_17-07-57.pth
--------------------------------------------------
train Epoch:31, Loss: 0.1233, lr: 0.00000100, Total running time: 46:9
Epoch: 32/1300
=======================================================
train Batch:14, loss:0.44810, batch running time: 0:54
train Batch:28, loss:0.34504, batch running time: 0:40
train Batch:42, loss:0.01426, batch running time: 0:28
train Batch:56, loss:0.04072, batch running time: 0:16
train Batch:70, loss:0.01098, batch running time: 0:02
train Batch:84, loss:0.07730, batch running time: 0:51
train Batch:98, loss:0.16693, batch running time: 0:38
train Batch:112, loss:0.01067, batch running time: 0:25
train Batch:126, loss:0.01346, batch running time: 0:11
train Batch:140, loss:0.00718, batch running time: 0:00
train Batch:154, loss:0.19696, batch running time: 0:47
train Batch:168, loss:0.11416, batch running time: 0:35
train Batch:182, loss:0.01483, batch running time: 0:23
train Batch:196, loss:0.00516, batch running time: 0:08
train Batch:210, loss:0.03080, batch running time: 0:51
train Batch:224, loss:0.26140, batch running time: 0:33
train Batch:238, loss:0.10879, batch running time: 0:20
train Batch:252, loss:0.07049, batch running time: 0:09
train Batch:266, loss:0.02997, batch running time: 0:57
train Batch:280, loss:0.00673, batch running time: 0:41
train Batch:294, loss:0.04909, batch running time: 0:25
train Batch:308, loss:0.00266, batch running time: 0:13
train Batch:322, loss:0.02378, batch running time: 0:59
train Batch:336, loss:0.01271, batch running time: 0:48
train Batch:350, loss:0.02458, batch running time: 0:32
train Batch:364, loss:0.03871, batch running time: 0:16
train Batch:378, loss:0.02728, batch running time: 0:04
train Batch:392, loss:0.06071, batch running time: 0:48
train Batch:406, loss:0.24891, batch running time: 0:30
train Batch:420, loss:0.02372, batch running time: 0:13
train Batch:434, loss:0.30426, batch running time: 0:03
train Batch:448, loss:0.06817, batch running time: 0:47
train Batch:462, loss:0.02152, batch running time: 0:36
train Batch:476, loss:0.09168, batch running time: 0:23
train Batch:490, loss:0.12621, batch running time: 0:12
train Batch:504, loss:0.00800, batch running time: 0:02
train Batch:518, loss:0.01496, batch running time: 0:52
train Batch:532, loss:0.12534, batch running time: 0:37
train Batch:546, loss:0.00488, batch running time: 0:18
train Batch:560, loss:0.02037, batch running time: 0:07
train Batch:574, loss:0.05587, batch running time: 0:47
train Batch:588, loss:0.01491, batch running time: 0:36
train Batch:602, loss:0.29523, batch running time: 0:21
train Batch:616, loss:0.18492, batch running time: 0:08
train Batch:630, loss:0.23955, batch running time: 0:52
train Batch:644, loss:0.26248, batch running time: 0:36
train Batch:658, loss:0.04391, batch running time: 0:22
train Batch:672, loss:0.09127, batch running time: 0:07
train Batch:686, loss:0.01293, batch running time: 0:54
train Batch:700, loss:0.10798, batch running time: 0:39
train Batch:714, loss:0.14331, batch running time: 0:24
train Batch:728, loss:0.03045, batch running time: 0:11
train Batch:742, loss:0.02742, batch running time: 0:01
train Batch:756, loss:0.02415, batch running time: 0:48
train Batch:770, loss:0.00261, batch running time: 0:37
train Batch:784, loss:0.01344, batch running time: 0:25
train Batch:798, loss:0.03844, batch running time: 0:13
train Batch:812, loss:0.06520, batch running time: 0:00
train Batch:826, loss:0.17824, batch running time: 0:46
train Batch:840, loss:0.01502, batch running time: 0:37
train Batch:854, loss:0.16125, batch running time: 0:23
train Batch:868, loss:0.14741, batch running time: 0:06
train Batch:882, loss:0.02835, batch running time: 0:54
train Batch:896, loss:0.04322, batch running time: 0:39
train Batch:910, loss:0.44192, batch running time: 0:24
train Batch:924, loss:0.06226, batch running time: 0:14
train Batch:938, loss:0.36007, batch running time: 0:00
train Batch:952, loss:0.15101, batch running time: 0:46
train Batch:966, loss:0.05726, batch running time: 0:32
train Batch:980, loss:0.54250, batch running time: 0:21
train Batch:994, loss:0.03419, batch running time: 0:07
train Batch:1008, loss:0.14295, batch running time: 0:53
train Batch:1022, loss:0.39759, batch running time: 0:42
train Batch:1036, loss:0.05838, batch running time: 0:29
train Batch:1050, loss:0.07186, batch running time: 0:18
train Batch:1064, loss:0.03174, batch running time: 0:04
train Batch:1078, loss:0.03941, batch running time: 0:53
train Batch:1092, loss:0.19088, batch running time: 0:36
train Batch:1106, loss:0.32990, batch running time: 0:18
train Batch:1120, loss:0.10686, batch running time: 0:07
train Batch:1134, loss:0.01619, batch running time: 0:55
train Batch:1148, loss:0.26634, batch running time: 0:42
train Batch:1162, loss:0.00334, batch running time: 0:29
train Batch:1176, loss:0.02051, batch running time: 0:15
train Batch:1190, loss:0.25124, batch running time: 0:00
train Batch:1204, loss:0.07162, batch running time: 0:46
train Batch:1218, loss:0.02722, batch running time: 0:30
train Batch:1232, loss:0.02511, batch running time: 0:10
train Batch:1246, loss:0.02921, batch running time: 0:57
train Batch:1260, loss:0.14952, batch running time: 0:45
train Batch:1274, loss:0.44765, batch running time: 0:29
train Batch:1288, loss:0.01762, batch running time: 0:13
train Batch:1302, loss:0.19847, batch running time: 0:55
train Batch:1316, loss:0.12585, batch running time: 0:42
train Batch:1330, loss:0.42044, batch running time: 0:24
train Batch:1344, loss:0.05298, batch running time: 0:11
train Batch:1358, loss:0.03349, batch running time: 0:57
--------------------------------------------------
train Epoch:32, Loss: 0.1300, lr: 0.00000100, Total running time: 48:40
Epoch: 33/1300
=======================================================
train Batch:14, loss:0.20510, batch running time: 0:54
train Batch:28, loss:0.01400, batch running time: 0:40
train Batch:42, loss:0.12048, batch running time: 0:29
train Batch:56, loss:0.02107, batch running time: 0:18
train Batch:70, loss:0.02971, batch running time: 0:04
train Batch:84, loss:0.02990, batch running time: 0:49
train Batch:98, loss:0.01572, batch running time: 0:38
train Batch:112, loss:0.11976, batch running time: 0:27
train Batch:126, loss:0.04349, batch running time: 0:15
train Batch:140, loss:0.03971, batch running time: 0:02
train Batch:154, loss:0.04626, batch running time: 0:49
train Batch:168, loss:0.09055, batch running time: 0:37
train Batch:182, loss:0.00371, batch running time: 0:27
train Batch:196, loss:0.04093, batch running time: 0:16
train Batch:210, loss:0.35069, batch running time: 0:04
train Batch:224, loss:0.01234, batch running time: 0:51
train Batch:238, loss:0.22615, batch running time: 0:38
train Batch:252, loss:0.01005, batch running time: 0:25
train Batch:266, loss:0.06094, batch running time: 0:11
train Batch:280, loss:0.25059, batch running time: 0:57
train Batch:294, loss:0.14574, batch running time: 0:42
train Batch:308, loss:0.02469, batch running time: 0:30
train Batch:322, loss:0.03797, batch running time: 0:13
train Batch:336, loss:0.27123, batch running time: 0:00
train Batch:350, loss:0.20437, batch running time: 0:48
train Batch:364, loss:0.04974, batch running time: 0:35
train Batch:378, loss:0.29315, batch running time: 0:20
train Batch:392, loss:0.06471, batch running time: 0:07
train Batch:406, loss:0.00690, batch running time: 0:54
train Batch:420, loss:0.14536, batch running time: 0:41
train Batch:434, loss:0.05221, batch running time: 0:26
train Batch:448, loss:0.02153, batch running time: 0:13
train Batch:462, loss:0.00544, batch running time: 0:00
train Batch:476, loss:0.01786, batch running time: 0:48
train Batch:490, loss:0.01476, batch running time: 0:32
train Batch:504, loss:0.00988, batch running time: 0:18
train Batch:518, loss:0.04118, batch running time: 0:06
train Batch:532, loss:0.01478, batch running time: 0:56
train Batch:546, loss:0.16305, batch running time: 0:42
train Batch:560, loss:0.02355, batch running time: 0:26
train Batch:574, loss:0.00420, batch running time: 0:14
train Batch:588, loss:0.07696, batch running time: 0:02
train Batch:602, loss:0.02856, batch running time: 0:47
train Batch:616, loss:0.01095, batch running time: 0:35
train Batch:630, loss:0.02661, batch running time: 0:21
train Batch:644, loss:0.15506, batch running time: 0:07
train Batch:658, loss:1.30903, batch running time: 0:47
train Batch:672, loss:0.07472, batch running time: 0:31
train Batch:686, loss:0.12511, batch running time: 0:16
train Batch:700, loss:0.10830, batch running time: 0:06
train Batch:714, loss:0.03180, batch running time: 0:52
train Batch:728, loss:0.14710, batch running time: 0:42
train Batch:742, loss:0.01020, batch running time: 0:25
train Batch:756, loss:0.25776, batch running time: 0:10
train Batch:770, loss:0.01538, batch running time: 0:56
train Batch:784, loss:0.15507, batch running time: 0:42
train Batch:798, loss:0.05057, batch running time: 0:29
train Batch:812, loss:0.03643, batch running time: 0:17
train Batch:826, loss:0.01303, batch running time: 0:00
train Batch:840, loss:0.10466, batch running time: 0:47
train Batch:854, loss:0.17078, batch running time: 0:33
train Batch:868, loss:0.16477, batch running time: 0:20
train Batch:882, loss:0.06347, batch running time: 0:06
train Batch:896, loss:0.04632, batch running time: 0:53
train Batch:910, loss:0.02750, batch running time: 0:41
train Batch:924, loss:0.27320, batch running time: 0:26
train Batch:938, loss:0.16109, batch running time: 0:08
./entrypoint.sh: line 4: 11955 Terminated              "$@" 2>&1
